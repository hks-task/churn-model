{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Objective of the notebook:\n",
    "\n",
    "We will try different parameter combinations for the final lightGBM classfier model to increase model performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-12T19:47:07.588101Z",
     "start_time": "2020-10-12T19:47:05.636185Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kaan.simsek/anaconda3/envs/forecasting/lib/python3.7/site-packages/lightgbm/__init__.py:46: UserWarning: Starting from version 2.2.1, the library file in distribution wheels for macOS is built by the Apple Clang (Xcode_8.3.3) compiler.\n",
      "This means that in case of installing LightGBM from PyPI via the ``pip install lightgbm`` command, you don't need to install the gcc compiler anymore.\n",
      "Instead of that, you need to install the OpenMP library, which is required for running LightGBM on the system with the Apple Clang compiler.\n",
      "You can install the OpenMP library by the following command: ``brew install libomp``.\n",
      "  \"You can install the OpenMP library by the following command: ``brew install libomp``.\", UserWarning)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import lightgbm \n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\") \n",
    "\n",
    "kfold = KFold(n_splits=5, random_state=42)\n",
    "break_point = datetime(2017, 2, 28)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-12T19:47:07.594708Z",
     "start_time": "2020-10-12T19:47:07.589970Z"
    }
   },
   "outputs": [],
   "source": [
    "def read_data():\n",
    "    \n",
    "    print('Reading files...')    \n",
    "    order_df = pd.read_csv('../input/machine_learning_challenge_order_data.csv')\n",
    "    print('Order data has {} rows and {} columns'.format(order_df.shape[0], order_df.shape[1]))\n",
    "    label_df = pd.read_csv('../input/machine_learning_challenge_labeled_data.csv')\n",
    "    print('Label data has {} rows and {} columns'.format(label_df.shape[0], label_df.shape[1]))\n",
    "    df = order_df.merge(label_df, on='customer_id')\n",
    "    print('The final data has {} rows and {} columns'.format(df.shape[0], df.shape[1]))\n",
    "    print(\"\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Change data types and reduce memory usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-12T19:47:07.602798Z",
     "start_time": "2020-10-12T19:47:07.596948Z"
    }
   },
   "outputs": [],
   "source": [
    "def reduce_mem_usage(df, verbose=False):\n",
    "    \n",
    "    start_mem = df.memory_usage().sum() / 1024 ** 2\n",
    "    int_columns = df.select_dtypes(include=[\"int\"]).columns\n",
    "    float_columns = df.select_dtypes(include=[\"float\"]).columns\n",
    "\n",
    "    for col in int_columns:\n",
    "        df[col] = pd.to_numeric(df[col], downcast=\"integer\")\n",
    "\n",
    "    for col in float_columns:\n",
    "        df[col] = pd.to_numeric(df[col], downcast=\"float\")\n",
    "\n",
    "    end_mem = df.memory_usage().sum() / 1024 ** 2\n",
    "    if verbose:\n",
    "        print(\n",
    "            \"Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)\".format(\n",
    "                end_mem, 100 * (start_mem - end_mem) / start_mem\n",
    "            )\n",
    "        )\n",
    "    print(\"\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Label encode categorical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-12T19:47:07.615153Z",
     "start_time": "2020-10-12T19:47:07.611697Z"
    }
   },
   "outputs": [],
   "source": [
    "def transform_data(df):\n",
    "\n",
    "    labelencoder = LabelEncoder()\n",
    "\n",
    "    for i in ['restaurant_id', 'city_id', 'payment_id', 'platform_id', 'transmission_id']:\n",
    "        df[i] = labelencoder.fit_transform(df[i])\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert raw data to a session format\n",
    "- Fill order rank with the forward-filling method. \n",
    "- Calculate recency and number of days from the first order.\n",
    "- Get time-related features like the year, month, week, day, day of the week, weekend.\n",
    "- Add day differences between consecutive orders. \n",
    "- Calculate rolling features in 3 days, 1, 2, 4, 12, 24 weeks, and all time.\n",
    "- Keep the last record of each customer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getWeeklyDates(df, break_point):\n",
    "\n",
    "    df['order_date'] = pd.to_datetime(df['order_date'])\n",
    "    three_day = df[df['order_date'] >= break_point - timedelta(days=3)]\n",
    "    one_week = df[df['order_date'] >= break_point - timedelta(days=7)]\n",
    "    two_week = df[df['order_date'] >= break_point - timedelta(days=14)]\n",
    "    four_week = df[df['order_date'] >= break_point - timedelta(days=28)]\n",
    "    twelve_week = df[df['order_date'] >= break_point - timedelta(days=84)]\n",
    "    twenty_four_week = df[df['order_date'] >= break_point - timedelta(days=168)]\n",
    "    all_week = df\n",
    "    return three_day, one_week, two_week, four_week, twelve_week, twenty_four_week,all_week"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-12T19:47:07.632315Z",
     "start_time": "2020-10-12T19:47:07.617325Z"
    }
   },
   "outputs": [],
   "source": [
    "def feature_engineering(df, break_point):\n",
    "    \n",
    "    df['customer_order_rank'] = df['customer_order_rank'].fillna(method='ffill')\n",
    "\n",
    "    df['date'] = pd.to_datetime(df['order_date']) \n",
    "    df['recency'] = (break_point - df['date']) / np.timedelta64(1, 'D')\n",
    "    df['first_order_date'] = df.groupby(['customer_id'])['date'].transform('first')\n",
    "    df['age_of_user'] = (break_point - df['first_order_date']) / np.timedelta64(1, 'D')\n",
    "\n",
    "    df['year'] = df['date'].dt.year\n",
    "    df['month'] = df['date'].dt.month\n",
    "    df['week'] = df['date'].dt.week\n",
    "    df['day'] = df['date'].dt.day\n",
    "    df['dayofweek'] = df['date'].dt.dayofweek\n",
    "    df[\"is_weekend\"] = df[\"dayofweek\"].isin([5, 6]).astype(np.int8)\n",
    "    \n",
    "    df['demand'] = 1\n",
    "    \n",
    "    df['order_date_shift'] = df.groupby('customer_id')['date'].shift()\n",
    "    df['date_diff'] = (df['date'] - df['order_date_shift']) / np.timedelta64(1, 'D')\n",
    "\n",
    "    three_day, one_week, two_week, four_week, twelve_week, twenty_four_week,all_week = getWeeklyDates(df, break_point)\n",
    "    \n",
    "    col = ['demand', 'is_failed','voucher_amount','delivery_fee', 'amount_paid', 'date_diff']\n",
    "    three_day, one_week, two_week, four_week, twelve_week, twenty_four_week,all_week = getWeeklyDates(df, break_point)\n",
    "    three_day = three_day.groupby('customer_id')[col].mean().add_prefix('three_day_').reset_index()\n",
    "    one_week = one_week.groupby('customer_id')[col].mean().add_prefix('one_week_').reset_index()\n",
    "    two_week = two_week.groupby('customer_id')[col].mean().add_prefix('two_week_').reset_index()\n",
    "    four_week = four_week.groupby('customer_id')[col].mean().add_prefix('four_week_').reset_index()\n",
    "    twelve_week = twelve_week.groupby('customer_id')[col].mean().add_prefix('twelve_week_').reset_index()\n",
    "    twenty_four_week = twenty_four_week.groupby('customer_id')[col].mean().add_prefix('twenty_four_week_').reset_index()\n",
    "    all_week = all_week.groupby('customer_id')[col].mean().add_prefix('all_week_').reset_index()\n",
    "    \n",
    "    df = df.groupby('customer_id').last().reset_index()\n",
    "    df = df.merge(three_day, how='left').merge(one_week, how='left').merge(two_week, how='left').merge(four_week,\n",
    "    'left').merge(twelve_week,'left').merge(twenty_four_week,'left').merge(all_week,'left').reset_index()\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Search best parameters for the final lightGBM model and features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-12T20:34:41.459997Z",
     "start_time": "2020-10-12T20:34:41.433580Z"
    }
   },
   "outputs": [],
   "source": [
    "def run_lgb(df):\n",
    "    \n",
    "    y = df['is_returning_customer']\n",
    "    X = df.drop(columns=['customer_id', 'order_date', 'date', 'is_returning_customer',\n",
    "                        'first_order_date', 'index', 'order_date_shift'])    \n",
    "    \n",
    "    clf = lightgbm.LGBMClassifier(n_jobs= -1, scale_pos_weight=2)\n",
    "    param_dist = {\n",
    "              'max_depth': np.arange(3,15,3) ,\n",
    "              'min_child_weight': np.arange(1,8,1),\n",
    "                      'colsample_bytree': np.arange(0.3,0.9,0.1),\n",
    "              'n_estimators': np.arange(100,1000,100),\n",
    "        'learning_rate'    : np.arange(0.05,0.3,0.05)\n",
    "        'num_leaves':  np.arange(10,100,10)\n",
    "             }\n",
    "    \n",
    "    fit_params={\"early_stopping_rounds\":10,\n",
    "               \"eval_metric\" : \"auc\", \n",
    "            \"eval_set\" : [[X, y]]}\n",
    "\n",
    "    grid_search = RandomizedSearchCV(clf, \n",
    "                         param_distributions = param_dist,\n",
    "                         cv = kfold,  \n",
    "                         n_iter = 50,\n",
    "                         verbose = 2, \n",
    "                         n_jobs = -1,\n",
    "                         fit_params=fit_params)\n",
    "    \n",
    "    grid_result = grid_search.fit(X,y)\n",
    "\n",
    "    print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "    means = grid_result.cv_results_[ 'mean_test_score' ]\n",
    "    stds = grid_result.cv_results_[ 'std_test_score' ]\n",
    "    params = grid_result.cv_results_[ 'params' ]\n",
    "    print(means, stds, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-10-12T20:34:41.925Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading files...\n",
      "Order data has 786600 rows and 13 columns\n",
      "Label data has 245455 rows and 2 columns\n",
      "The final data has 786600 rows and 14 columns\n",
      "\n",
      "Mem. usage decreased to 42.76 Mb (52.5% reduction)\n",
      "\n",
      "Fitting 5 folds for each of 50 candidates, totalling 250 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  17 tasks      | elapsed:  3.2min\n",
      "[Parallel(n_jobs=-1)]: Done 138 tasks      | elapsed: 25.6min\n",
      "[Parallel(n_jobs=-1)]: Done 250 out of 250 | elapsed: 46.5min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\ttraining's auc: 0.807842\ttraining's binary_logloss: 0.519573\n",
      "Training until validation scores don't improve for 10 rounds.\n",
      "[2]\ttraining's auc: 0.814547\ttraining's binary_logloss: 0.506985\n",
      "[3]\ttraining's auc: 0.816115\ttraining's binary_logloss: 0.496391\n",
      "[4]\ttraining's auc: 0.816562\ttraining's binary_logloss: 0.487424\n",
      "[5]\ttraining's auc: 0.816943\ttraining's binary_logloss: 0.47965\n",
      "[6]\ttraining's auc: 0.816978\ttraining's binary_logloss: 0.472978\n",
      "[7]\ttraining's auc: 0.816993\ttraining's binary_logloss: 0.467171\n",
      "[8]\ttraining's auc: 0.817157\ttraining's binary_logloss: 0.462094\n",
      "[9]\ttraining's auc: 0.817291\ttraining's binary_logloss: 0.457536\n",
      "[10]\ttraining's auc: 0.817382\ttraining's binary_logloss: 0.453509\n",
      "[11]\ttraining's auc: 0.817437\ttraining's binary_logloss: 0.449956\n",
      "[12]\ttraining's auc: 0.81747\ttraining's binary_logloss: 0.446815\n",
      "[13]\ttraining's auc: 0.817596\ttraining's binary_logloss: 0.443971\n",
      "[14]\ttraining's auc: 0.817708\ttraining's binary_logloss: 0.441448\n",
      "[15]\ttraining's auc: 0.817803\ttraining's binary_logloss: 0.439182\n",
      "[16]\ttraining's auc: 0.817856\ttraining's binary_logloss: 0.437204\n",
      "[17]\ttraining's auc: 0.81798\ttraining's binary_logloss: 0.43539\n",
      "[18]\ttraining's auc: 0.818041\ttraining's binary_logloss: 0.433756\n",
      "[19]\ttraining's auc: 0.818156\ttraining's binary_logloss: 0.432338\n",
      "[20]\ttraining's auc: 0.81823\ttraining's binary_logloss: 0.431092\n",
      "[21]\ttraining's auc: 0.818288\ttraining's binary_logloss: 0.429912\n",
      "[22]\ttraining's auc: 0.818393\ttraining's binary_logloss: 0.428881\n",
      "[23]\ttraining's auc: 0.818441\ttraining's binary_logloss: 0.427947\n",
      "[24]\ttraining's auc: 0.818492\ttraining's binary_logloss: 0.42712\n",
      "[25]\ttraining's auc: 0.818552\ttraining's binary_logloss: 0.426386\n",
      "[26]\ttraining's auc: 0.818615\ttraining's binary_logloss: 0.425709\n",
      "[27]\ttraining's auc: 0.818722\ttraining's binary_logloss: 0.425126\n",
      "[28]\ttraining's auc: 0.818768\ttraining's binary_logloss: 0.424599\n",
      "[29]\ttraining's auc: 0.818833\ttraining's binary_logloss: 0.424159\n",
      "[30]\ttraining's auc: 0.818877\ttraining's binary_logloss: 0.423736\n",
      "[31]\ttraining's auc: 0.818932\ttraining's binary_logloss: 0.423361\n",
      "[32]\ttraining's auc: 0.818997\ttraining's binary_logloss: 0.423025\n",
      "[33]\ttraining's auc: 0.819062\ttraining's binary_logloss: 0.42274\n",
      "[34]\ttraining's auc: 0.819127\ttraining's binary_logloss: 0.4225\n",
      "[35]\ttraining's auc: 0.819191\ttraining's binary_logloss: 0.422278\n",
      "[36]\ttraining's auc: 0.81926\ttraining's binary_logloss: 0.42209\n",
      "[37]\ttraining's auc: 0.819302\ttraining's binary_logloss: 0.421919\n",
      "[38]\ttraining's auc: 0.819351\ttraining's binary_logloss: 0.421784\n",
      "[39]\ttraining's auc: 0.819411\ttraining's binary_logloss: 0.421653\n",
      "[40]\ttraining's auc: 0.819469\ttraining's binary_logloss: 0.421546\n",
      "[41]\ttraining's auc: 0.819505\ttraining's binary_logloss: 0.421459\n",
      "[42]\ttraining's auc: 0.81956\ttraining's binary_logloss: 0.421384\n",
      "[43]\ttraining's auc: 0.819658\ttraining's binary_logloss: 0.421314\n",
      "[44]\ttraining's auc: 0.819725\ttraining's binary_logloss: 0.421259\n",
      "[45]\ttraining's auc: 0.819773\ttraining's binary_logloss: 0.421211\n",
      "[46]\ttraining's auc: 0.819832\ttraining's binary_logloss: 0.421172\n",
      "[47]\ttraining's auc: 0.819927\ttraining's binary_logloss: 0.421128\n",
      "[48]\ttraining's auc: 0.820017\ttraining's binary_logloss: 0.421087\n",
      "[49]\ttraining's auc: 0.820078\ttraining's binary_logloss: 0.421065\n",
      "[50]\ttraining's auc: 0.820155\ttraining's binary_logloss: 0.42104\n",
      "[51]\ttraining's auc: 0.820219\ttraining's binary_logloss: 0.421022\n",
      "[52]\ttraining's auc: 0.820278\ttraining's binary_logloss: 0.421019\n",
      "[53]\ttraining's auc: 0.820367\ttraining's binary_logloss: 0.420992\n",
      "[54]\ttraining's auc: 0.820449\ttraining's binary_logloss: 0.420977\n",
      "[55]\ttraining's auc: 0.820533\ttraining's binary_logloss: 0.420975\n",
      "[56]\ttraining's auc: 0.820631\ttraining's binary_logloss: 0.420967\n",
      "[57]\ttraining's auc: 0.820768\ttraining's binary_logloss: 0.420954\n"
     ]
    }
   ],
   "source": [
    "def transform_train_and_eval():\n",
    "    \n",
    "    df = read_data()\n",
    "    df = reduce_mem_usage(df, True)\n",
    "    df = transform_data(df)\n",
    "    df = feature_engineering(df, break_point)\n",
    "    predicted = run_lgb(df)\n",
    "    return predicted\n",
    "\n",
    "predicted = transform_train_and_eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
