{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Objective of the notebook:\n",
    "\n",
    "In this notebook, we will try different parameter combinations for the final lightGBM classfier model to increase model performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-13T10:07:17.545572Z",
     "start_time": "2020-10-13T10:07:15.303718Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kaan.simsek/anaconda3/envs/forecasting/lib/python3.7/site-packages/lightgbm/__init__.py:46: UserWarning: Starting from version 2.2.1, the library file in distribution wheels for macOS is built by the Apple Clang (Xcode_8.3.3) compiler.\n",
      "This means that in case of installing LightGBM from PyPI via the ``pip install lightgbm`` command, you don't need to install the gcc compiler anymore.\n",
      "Instead of that, you need to install the OpenMP library, which is required for running LightGBM on the system with the Apple Clang compiler.\n",
      "You can install the OpenMP library by the following command: ``brew install libomp``.\n",
      "  \"You can install the OpenMP library by the following command: ``brew install libomp``.\", UserWarning)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import lightgbm \n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\") \n",
    "\n",
    "kfold = KFold(n_splits=5, random_state=42)\n",
    "break_point = datetime(2017, 2, 28)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-13T10:07:17.554791Z",
     "start_time": "2020-10-13T10:07:17.548258Z"
    }
   },
   "outputs": [],
   "source": [
    "def read_data():\n",
    "    \n",
    "    print('Reading files...')    \n",
    "    order_df = pd.read_csv('../input/machine_learning_challenge_order_data.csv')\n",
    "    print('Order data has {} rows and {} columns'.format(order_df.shape[0], order_df.shape[1]))\n",
    "    label_df = pd.read_csv('../input/machine_learning_challenge_labeled_data.csv')\n",
    "    print('Label data has {} rows and {} columns'.format(label_df.shape[0], label_df.shape[1]))\n",
    "    df = order_df.merge(label_df, on='customer_id')\n",
    "    print('The final data has {} rows and {} columns'.format(df.shape[0], df.shape[1]))\n",
    "    print(\"\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Change data types and reduce memory usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-13T10:07:17.566057Z",
     "start_time": "2020-10-13T10:07:17.557429Z"
    }
   },
   "outputs": [],
   "source": [
    "def reduce_mem_usage(df, verbose=False):\n",
    "    \n",
    "    start_mem = df.memory_usage().sum() / 1024 ** 2\n",
    "    int_columns = df.select_dtypes(include=[\"int\"]).columns\n",
    "    float_columns = df.select_dtypes(include=[\"float\"]).columns\n",
    "\n",
    "    for col in int_columns:\n",
    "        df[col] = pd.to_numeric(df[col], downcast=\"integer\")\n",
    "\n",
    "    for col in float_columns:\n",
    "        df[col] = pd.to_numeric(df[col], downcast=\"float\")\n",
    "\n",
    "    end_mem = df.memory_usage().sum() / 1024 ** 2\n",
    "    if verbose:\n",
    "        print(\n",
    "            \"Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)\".format(\n",
    "                end_mem, 100 * (start_mem - end_mem) / start_mem\n",
    "            )\n",
    "        )\n",
    "    print(\"\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Label encode categorical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-13T10:07:17.572883Z",
     "start_time": "2020-10-13T10:07:17.568475Z"
    }
   },
   "outputs": [],
   "source": [
    "def transform_data(df):\n",
    "\n",
    "    labelencoder = LabelEncoder()\n",
    "\n",
    "    for i in ['restaurant_id', 'city_id', 'payment_id', 'platform_id', 'transmission_id']:\n",
    "        df[i] = labelencoder.fit_transform(df[i])\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert raw data to a session format\n",
    "- Fill order rank with the forward-filling method. \n",
    "- Calculate recency and number of days from the first order.\n",
    "- Get time-related features like the year, month, week, day, day of the week, weekend.\n",
    "- Add day differences between consecutive orders. \n",
    "- Calculate rolling features in 3 days, 1, 2, 4, 12, 24 weeks, and all time.\n",
    "- Keep the last record of each customer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-13T10:07:17.584989Z",
     "start_time": "2020-10-13T10:07:17.575664Z"
    }
   },
   "outputs": [],
   "source": [
    "def getWeeklyDates(df, break_point):\n",
    "\n",
    "    df['order_date'] = pd.to_datetime(df['order_date'])\n",
    "    three_day = df[df['order_date'] >= break_point - timedelta(days=3)]\n",
    "    one_week = df[df['order_date'] >= break_point - timedelta(days=7)]\n",
    "    two_week = df[df['order_date'] >= break_point - timedelta(days=14)]\n",
    "    four_week = df[df['order_date'] >= break_point - timedelta(days=28)]\n",
    "    twelve_week = df[df['order_date'] >= break_point - timedelta(days=84)]\n",
    "    twenty_four_week = df[df['order_date'] >= break_point - timedelta(days=168)]\n",
    "    all_week = df\n",
    "    return three_day, one_week, two_week, four_week, twelve_week, twenty_four_week,all_week"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-13T10:07:17.608328Z",
     "start_time": "2020-10-13T10:07:17.590000Z"
    }
   },
   "outputs": [],
   "source": [
    "def feature_engineering(df, break_point):\n",
    "    \n",
    "    df['customer_order_rank'] = df['customer_order_rank'].fillna(method='ffill')\n",
    "\n",
    "    df['date'] = pd.to_datetime(df['order_date']) \n",
    "    df['recency'] = (break_point - df['date']) / np.timedelta64(1, 'D')\n",
    "    df['first_order_date'] = df.groupby(['customer_id'])['date'].transform('first')\n",
    "    df['age_of_user'] = (break_point - df['first_order_date']) / np.timedelta64(1, 'D')\n",
    "\n",
    "    df['year'] = df['date'].dt.year\n",
    "    df['month'] = df['date'].dt.month\n",
    "    df['week'] = df['date'].dt.week\n",
    "    df['day'] = df['date'].dt.day\n",
    "    df['dayofweek'] = df['date'].dt.dayofweek\n",
    "    df[\"is_weekend\"] = df[\"dayofweek\"].isin([5, 6]).astype(np.int8)\n",
    "    \n",
    "    df['demand'] = 1\n",
    "    \n",
    "    df['order_date_shift'] = df.groupby('customer_id')['date'].shift()\n",
    "    df['date_diff'] = (df['date'] - df['order_date_shift']) / np.timedelta64(1, 'D')\n",
    "\n",
    "    three_day, one_week, two_week, four_week, twelve_week, twenty_four_week,all_week = getWeeklyDates(df, break_point)\n",
    "    \n",
    "    col = ['demand', 'is_failed','voucher_amount','delivery_fee', 'amount_paid', 'date_diff']\n",
    "    three_day, one_week, two_week, four_week, twelve_week, twenty_four_week,all_week = getWeeklyDates(df, break_point)\n",
    "    three_day = three_day.groupby('customer_id')[col].mean().add_prefix('three_day_').reset_index()\n",
    "    one_week = one_week.groupby('customer_id')[col].mean().add_prefix('one_week_').reset_index()\n",
    "    two_week = two_week.groupby('customer_id')[col].mean().add_prefix('two_week_').reset_index()\n",
    "    four_week = four_week.groupby('customer_id')[col].mean().add_prefix('four_week_').reset_index()\n",
    "    twelve_week = twelve_week.groupby('customer_id')[col].mean().add_prefix('twelve_week_').reset_index()\n",
    "    twenty_four_week = twenty_four_week.groupby('customer_id')[col].mean().add_prefix('twenty_four_week_').reset_index()\n",
    "    all_week = all_week.groupby('customer_id')[col].mean().add_prefix('all_week_').reset_index()\n",
    "    \n",
    "    df = df.groupby('customer_id').last().reset_index()\n",
    "    df = df.merge(three_day, how='left').merge(one_week, how='left').merge(two_week, how='left').merge(four_week,\n",
    "    'left').merge(twelve_week,'left').merge(twenty_four_week,'left').merge(all_week,'left').reset_index()\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Search best parameters for the final lightGBM model and features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-13T10:07:17.623870Z",
     "start_time": "2020-10-13T10:07:17.610357Z"
    }
   },
   "outputs": [],
   "source": [
    "def run_lgb(df):\n",
    "    \n",
    "    y = df['is_returning_customer']\n",
    "    X = df.drop(columns=['customer_id', 'order_date', 'date', 'is_returning_customer',\n",
    "                        'first_order_date', 'index', 'order_date_shift'])    \n",
    "    \n",
    "    clf = lightgbm.LGBMClassifier(n_jobs= -1, scale_pos_weight=2)\n",
    "    \n",
    "    param_dist = {\n",
    "                    'max_depth': np.arange(3,15,3),\n",
    "                    'min_child_weight': np.arange(1,8,1),\n",
    "                    'colsample_bytree': np.arange(0.3,0.9,0.1),\n",
    "                    'n_estimators': np.arange(100,1000,100),\n",
    "                    'learning_rate': np.arange(0.05,0.3,0.05),\n",
    "                    'num_leaves':  np.arange(10,100,10)\n",
    "             }\n",
    "    \n",
    "    fit_params={\"early_stopping_rounds\":10,\n",
    "               \"eval_metric\" : \"auc\", \n",
    "               \"eval_set\" : [[X, y]]}\n",
    "\n",
    "    grid_search = RandomizedSearchCV(clf, \n",
    "                         param_distributions = param_dist,\n",
    "                         cv = kfold,  \n",
    "                         n_iter = 50,\n",
    "                         verbose = 0, \n",
    "                         n_jobs = -1,\n",
    "                         fit_params=fit_params)\n",
    "    \n",
    "    grid_result = grid_search.fit(X,y)\n",
    "\n",
    "    print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "    means = grid_result.cv_results_[ 'mean_test_score' ]\n",
    "    stds = grid_result.cv_results_[ 'std_test_score' ]\n",
    "    params = grid_result.cv_results_[ 'params' ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-13T11:01:06.219448Z",
     "start_time": "2020-10-13T10:07:17.626499Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading files...\n",
      "Order data has 786600 rows and 13 columns\n",
      "Label data has 245455 rows and 2 columns\n",
      "The final data has 786600 rows and 14 columns\n",
      "\n",
      "Mem. usage decreased to 42.76 Mb (52.5% reduction)\n",
      "\n",
      "Fitting 5 folds for each of 50 candidates, totalling 250 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  26 tasks      | elapsed:  8.0min\n",
      "[Parallel(n_jobs=-1)]: Done 176 tasks      | elapsed: 35.7min\n",
      "[Parallel(n_jobs=-1)]: Done 250 out of 250 | elapsed: 53.2min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\ttraining's binary_logloss: 0.519346\ttraining's auc: 0.812477\n",
      "Training until validation scores don't improve for 10 rounds.\n",
      "[2]\ttraining's binary_logloss: 0.506669\ttraining's auc: 0.816096\n",
      "[3]\ttraining's binary_logloss: 0.496034\ttraining's auc: 0.81683\n",
      "[4]\ttraining's binary_logloss: 0.48717\ttraining's auc: 0.817118\n",
      "[5]\ttraining's binary_logloss: 0.47935\ttraining's auc: 0.817511\n",
      "[6]\ttraining's binary_logloss: 0.472704\ttraining's auc: 0.81774\n",
      "[7]\ttraining's binary_logloss: 0.466823\ttraining's auc: 0.817878\n",
      "[8]\ttraining's binary_logloss: 0.461705\ttraining's auc: 0.818008\n",
      "[9]\ttraining's binary_logloss: 0.457119\ttraining's auc: 0.818308\n",
      "[10]\ttraining's binary_logloss: 0.453052\ttraining's auc: 0.818426\n",
      "[11]\ttraining's binary_logloss: 0.449482\ttraining's auc: 0.818492\n",
      "[12]\ttraining's binary_logloss: 0.446303\ttraining's auc: 0.818648\n",
      "[13]\ttraining's binary_logloss: 0.443463\ttraining's auc: 0.818838\n",
      "[14]\ttraining's binary_logloss: 0.440929\ttraining's auc: 0.818916\n",
      "[15]\ttraining's binary_logloss: 0.43864\ttraining's auc: 0.818937\n",
      "[16]\ttraining's binary_logloss: 0.436641\ttraining's auc: 0.819023\n",
      "[17]\ttraining's binary_logloss: 0.434809\ttraining's auc: 0.819117\n",
      "[18]\ttraining's binary_logloss: 0.433152\ttraining's auc: 0.819208\n",
      "[19]\ttraining's binary_logloss: 0.431738\ttraining's auc: 0.819287\n",
      "[20]\ttraining's binary_logloss: 0.430452\ttraining's auc: 0.819345\n",
      "[21]\ttraining's binary_logloss: 0.429273\ttraining's auc: 0.819442\n",
      "[22]\ttraining's binary_logloss: 0.428231\ttraining's auc: 0.819546\n",
      "[23]\ttraining's binary_logloss: 0.427291\ttraining's auc: 0.819671\n",
      "[24]\ttraining's binary_logloss: 0.426446\ttraining's auc: 0.819732\n",
      "[25]\ttraining's binary_logloss: 0.425707\ttraining's auc: 0.819829\n",
      "[26]\ttraining's binary_logloss: 0.425014\ttraining's auc: 0.81989\n",
      "[27]\ttraining's binary_logloss: 0.424432\ttraining's auc: 0.819943\n",
      "[28]\ttraining's binary_logloss: 0.423885\ttraining's auc: 0.820046\n",
      "[29]\ttraining's binary_logloss: 0.423433\ttraining's auc: 0.820117\n",
      "[30]\ttraining's binary_logloss: 0.422997\ttraining's auc: 0.820205\n",
      "[31]\ttraining's binary_logloss: 0.422617\ttraining's auc: 0.82026\n",
      "[32]\ttraining's binary_logloss: 0.422264\ttraining's auc: 0.820377\n",
      "[33]\ttraining's binary_logloss: 0.42196\ttraining's auc: 0.820433\n",
      "[34]\ttraining's binary_logloss: 0.421716\ttraining's auc: 0.820494\n",
      "[35]\ttraining's binary_logloss: 0.421501\ttraining's auc: 0.82055\n",
      "[36]\ttraining's binary_logloss: 0.421305\ttraining's auc: 0.82063\n",
      "[37]\ttraining's binary_logloss: 0.421129\ttraining's auc: 0.820699\n",
      "[38]\ttraining's binary_logloss: 0.420975\ttraining's auc: 0.820778\n",
      "[39]\ttraining's binary_logloss: 0.42082\ttraining's auc: 0.820872\n",
      "[40]\ttraining's binary_logloss: 0.420687\ttraining's auc: 0.820954\n",
      "[41]\ttraining's binary_logloss: 0.420572\ttraining's auc: 0.821014\n",
      "[42]\ttraining's binary_logloss: 0.420482\ttraining's auc: 0.821086\n",
      "[43]\ttraining's binary_logloss: 0.420416\ttraining's auc: 0.82114\n",
      "[44]\ttraining's binary_logloss: 0.420347\ttraining's auc: 0.821231\n",
      "[45]\ttraining's binary_logloss: 0.420279\ttraining's auc: 0.821307\n",
      "[46]\ttraining's binary_logloss: 0.42022\ttraining's auc: 0.821364\n",
      "[47]\ttraining's binary_logloss: 0.420178\ttraining's auc: 0.821428\n",
      "[48]\ttraining's binary_logloss: 0.42015\ttraining's auc: 0.8215\n",
      "[49]\ttraining's binary_logloss: 0.420124\ttraining's auc: 0.821555\n",
      "[50]\ttraining's binary_logloss: 0.420092\ttraining's auc: 0.821641\n",
      "[51]\ttraining's binary_logloss: 0.420055\ttraining's auc: 0.821721\n",
      "[52]\ttraining's binary_logloss: 0.420041\ttraining's auc: 0.821799\n",
      "[53]\ttraining's binary_logloss: 0.419998\ttraining's auc: 0.821873\n",
      "[54]\ttraining's binary_logloss: 0.419978\ttraining's auc: 0.821948\n",
      "[55]\ttraining's binary_logloss: 0.419967\ttraining's auc: 0.822018\n",
      "[56]\ttraining's binary_logloss: 0.419947\ttraining's auc: 0.822124\n",
      "[57]\ttraining's binary_logloss: 0.419945\ttraining's auc: 0.82219\n",
      "[58]\ttraining's binary_logloss: 0.419908\ttraining's auc: 0.822278\n",
      "[59]\ttraining's binary_logloss: 0.419885\ttraining's auc: 0.822362\n",
      "[60]\ttraining's binary_logloss: 0.419869\ttraining's auc: 0.822415\n",
      "[61]\ttraining's binary_logloss: 0.419868\ttraining's auc: 0.822503\n",
      "[62]\ttraining's binary_logloss: 0.419836\ttraining's auc: 0.822598\n",
      "[63]\ttraining's binary_logloss: 0.419814\ttraining's auc: 0.82269\n",
      "[64]\ttraining's binary_logloss: 0.419789\ttraining's auc: 0.822747\n",
      "[65]\ttraining's binary_logloss: 0.419769\ttraining's auc: 0.822815\n",
      "[66]\ttraining's binary_logloss: 0.419746\ttraining's auc: 0.822889\n",
      "[67]\ttraining's binary_logloss: 0.419729\ttraining's auc: 0.822969\n",
      "[68]\ttraining's binary_logloss: 0.41971\ttraining's auc: 0.823055\n",
      "[69]\ttraining's binary_logloss: 0.419685\ttraining's auc: 0.823124\n",
      "[70]\ttraining's binary_logloss: 0.41966\ttraining's auc: 0.823195\n",
      "[71]\ttraining's binary_logloss: 0.419637\ttraining's auc: 0.823261\n",
      "[72]\ttraining's binary_logloss: 0.419619\ttraining's auc: 0.823334\n",
      "[73]\ttraining's binary_logloss: 0.419595\ttraining's auc: 0.823397\n",
      "[74]\ttraining's binary_logloss: 0.419563\ttraining's auc: 0.823492\n",
      "[75]\ttraining's binary_logloss: 0.419538\ttraining's auc: 0.823542\n",
      "[76]\ttraining's binary_logloss: 0.419516\ttraining's auc: 0.823606\n",
      "[77]\ttraining's binary_logloss: 0.419484\ttraining's auc: 0.823679\n",
      "[78]\ttraining's binary_logloss: 0.41945\ttraining's auc: 0.823726\n",
      "[79]\ttraining's binary_logloss: 0.419418\ttraining's auc: 0.82379\n",
      "[80]\ttraining's binary_logloss: 0.419387\ttraining's auc: 0.823858\n",
      "[81]\ttraining's binary_logloss: 0.419352\ttraining's auc: 0.823927\n",
      "[82]\ttraining's binary_logloss: 0.419316\ttraining's auc: 0.82399\n",
      "[83]\ttraining's binary_logloss: 0.419277\ttraining's auc: 0.824066\n",
      "[84]\ttraining's binary_logloss: 0.41924\ttraining's auc: 0.824111\n",
      "[85]\ttraining's binary_logloss: 0.419206\ttraining's auc: 0.824146\n",
      "[86]\ttraining's binary_logloss: 0.419177\ttraining's auc: 0.824217\n",
      "[87]\ttraining's binary_logloss: 0.419127\ttraining's auc: 0.824283\n",
      "[88]\ttraining's binary_logloss: 0.419076\ttraining's auc: 0.824377\n",
      "[89]\ttraining's binary_logloss: 0.419032\ttraining's auc: 0.824451\n",
      "[90]\ttraining's binary_logloss: 0.418987\ttraining's auc: 0.824519\n",
      "[91]\ttraining's binary_logloss: 0.418945\ttraining's auc: 0.824604\n",
      "[92]\ttraining's binary_logloss: 0.418895\ttraining's auc: 0.824657\n",
      "[93]\ttraining's binary_logloss: 0.418846\ttraining's auc: 0.824712\n",
      "[94]\ttraining's binary_logloss: 0.418797\ttraining's auc: 0.824764\n",
      "[95]\ttraining's binary_logloss: 0.418746\ttraining's auc: 0.824832\n",
      "[96]\ttraining's binary_logloss: 0.418699\ttraining's auc: 0.824878\n",
      "[97]\ttraining's binary_logloss: 0.418654\ttraining's auc: 0.824927\n",
      "[98]\ttraining's binary_logloss: 0.418606\ttraining's auc: 0.824973\n",
      "[99]\ttraining's binary_logloss: 0.41856\ttraining's auc: 0.825014\n",
      "[100]\ttraining's binary_logloss: 0.418522\ttraining's auc: 0.825054\n",
      "[101]\ttraining's binary_logloss: 0.418478\ttraining's auc: 0.825091\n",
      "[102]\ttraining's binary_logloss: 0.41842\ttraining's auc: 0.825174\n",
      "[103]\ttraining's binary_logloss: 0.418367\ttraining's auc: 0.825244\n",
      "[104]\ttraining's binary_logloss: 0.418324\ttraining's auc: 0.825286\n",
      "[105]\ttraining's binary_logloss: 0.418272\ttraining's auc: 0.825351\n",
      "[106]\ttraining's binary_logloss: 0.418225\ttraining's auc: 0.825419\n",
      "[107]\ttraining's binary_logloss: 0.418177\ttraining's auc: 0.825497\n",
      "[108]\ttraining's binary_logloss: 0.418136\ttraining's auc: 0.825548\n",
      "[109]\ttraining's binary_logloss: 0.418089\ttraining's auc: 0.825644\n",
      "[110]\ttraining's binary_logloss: 0.418027\ttraining's auc: 0.825702\n",
      "[111]\ttraining's binary_logloss: 0.417974\ttraining's auc: 0.825788\n",
      "[112]\ttraining's binary_logloss: 0.417923\ttraining's auc: 0.825823\n",
      "[113]\ttraining's binary_logloss: 0.417871\ttraining's auc: 0.825869\n",
      "[114]\ttraining's binary_logloss: 0.417821\ttraining's auc: 0.825936\n",
      "[115]\ttraining's binary_logloss: 0.417757\ttraining's auc: 0.825987\n",
      "[116]\ttraining's binary_logloss: 0.417707\ttraining's auc: 0.826058\n",
      "[117]\ttraining's binary_logloss: 0.41765\ttraining's auc: 0.826118\n",
      "[118]\ttraining's binary_logloss: 0.417589\ttraining's auc: 0.826196\n",
      "[119]\ttraining's binary_logloss: 0.417537\ttraining's auc: 0.826255\n",
      "[120]\ttraining's binary_logloss: 0.41748\ttraining's auc: 0.82631\n",
      "[121]\ttraining's binary_logloss: 0.417419\ttraining's auc: 0.826394\n",
      "[122]\ttraining's binary_logloss: 0.417363\ttraining's auc: 0.826472\n",
      "[123]\ttraining's binary_logloss: 0.417309\ttraining's auc: 0.826512\n",
      "[124]\ttraining's binary_logloss: 0.417247\ttraining's auc: 0.826566\n",
      "[125]\ttraining's binary_logloss: 0.417199\ttraining's auc: 0.826655\n",
      "[126]\ttraining's binary_logloss: 0.417154\ttraining's auc: 0.826725\n",
      "[127]\ttraining's binary_logloss: 0.417106\ttraining's auc: 0.826797\n",
      "[128]\ttraining's binary_logloss: 0.417052\ttraining's auc: 0.826868\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[129]\ttraining's binary_logloss: 0.417009\ttraining's auc: 0.826923\n",
      "[130]\ttraining's binary_logloss: 0.416953\ttraining's auc: 0.826996\n",
      "[131]\ttraining's binary_logloss: 0.416901\ttraining's auc: 0.827061\n",
      "[132]\ttraining's binary_logloss: 0.416842\ttraining's auc: 0.827133\n",
      "[133]\ttraining's binary_logloss: 0.416784\ttraining's auc: 0.827212\n",
      "[134]\ttraining's binary_logloss: 0.416733\ttraining's auc: 0.827257\n",
      "[135]\ttraining's binary_logloss: 0.416676\ttraining's auc: 0.827314\n",
      "[136]\ttraining's binary_logloss: 0.416627\ttraining's auc: 0.827369\n",
      "[137]\ttraining's binary_logloss: 0.416574\ttraining's auc: 0.827405\n",
      "[138]\ttraining's binary_logloss: 0.416514\ttraining's auc: 0.827485\n",
      "[139]\ttraining's binary_logloss: 0.416473\ttraining's auc: 0.827544\n",
      "[140]\ttraining's binary_logloss: 0.416421\ttraining's auc: 0.827587\n",
      "[141]\ttraining's binary_logloss: 0.416362\ttraining's auc: 0.827639\n",
      "[142]\ttraining's binary_logloss: 0.416294\ttraining's auc: 0.827714\n",
      "[143]\ttraining's binary_logloss: 0.41624\ttraining's auc: 0.827765\n",
      "[144]\ttraining's binary_logloss: 0.416201\ttraining's auc: 0.82779\n",
      "[145]\ttraining's binary_logloss: 0.416157\ttraining's auc: 0.827849\n",
      "[146]\ttraining's binary_logloss: 0.416113\ttraining's auc: 0.827888\n",
      "[147]\ttraining's binary_logloss: 0.416047\ttraining's auc: 0.827969\n",
      "[148]\ttraining's binary_logloss: 0.415999\ttraining's auc: 0.82801\n",
      "[149]\ttraining's binary_logloss: 0.415942\ttraining's auc: 0.828072\n",
      "[150]\ttraining's binary_logloss: 0.415891\ttraining's auc: 0.828115\n",
      "[151]\ttraining's binary_logloss: 0.415836\ttraining's auc: 0.828181\n",
      "[152]\ttraining's binary_logloss: 0.415786\ttraining's auc: 0.828237\n",
      "[153]\ttraining's binary_logloss: 0.415726\ttraining's auc: 0.82831\n",
      "[154]\ttraining's binary_logloss: 0.415669\ttraining's auc: 0.828359\n",
      "[155]\ttraining's binary_logloss: 0.415629\ttraining's auc: 0.828414\n",
      "[156]\ttraining's binary_logloss: 0.415572\ttraining's auc: 0.828477\n",
      "[157]\ttraining's binary_logloss: 0.415523\ttraining's auc: 0.828512\n",
      "[158]\ttraining's binary_logloss: 0.415466\ttraining's auc: 0.828569\n",
      "[159]\ttraining's binary_logloss: 0.415403\ttraining's auc: 0.828649\n",
      "[160]\ttraining's binary_logloss: 0.41535\ttraining's auc: 0.828693\n",
      "[161]\ttraining's binary_logloss: 0.415299\ttraining's auc: 0.828742\n",
      "[162]\ttraining's binary_logloss: 0.415243\ttraining's auc: 0.828814\n",
      "[163]\ttraining's binary_logloss: 0.415198\ttraining's auc: 0.828871\n",
      "[164]\ttraining's binary_logloss: 0.415148\ttraining's auc: 0.828926\n",
      "[165]\ttraining's binary_logloss: 0.415084\ttraining's auc: 0.828957\n",
      "[166]\ttraining's binary_logloss: 0.415037\ttraining's auc: 0.82902\n",
      "[167]\ttraining's binary_logloss: 0.414991\ttraining's auc: 0.829068\n",
      "[168]\ttraining's binary_logloss: 0.414939\ttraining's auc: 0.829111\n",
      "[169]\ttraining's binary_logloss: 0.414891\ttraining's auc: 0.829173\n",
      "[170]\ttraining's binary_logloss: 0.414847\ttraining's auc: 0.829206\n",
      "[171]\ttraining's binary_logloss: 0.414789\ttraining's auc: 0.829276\n",
      "[172]\ttraining's binary_logloss: 0.41474\ttraining's auc: 0.829312\n",
      "[173]\ttraining's binary_logloss: 0.414688\ttraining's auc: 0.829346\n",
      "[174]\ttraining's binary_logloss: 0.414642\ttraining's auc: 0.829394\n",
      "[175]\ttraining's binary_logloss: 0.414608\ttraining's auc: 0.829443\n",
      "[176]\ttraining's binary_logloss: 0.41455\ttraining's auc: 0.829476\n",
      "[177]\ttraining's binary_logloss: 0.414508\ttraining's auc: 0.829524\n",
      "[178]\ttraining's binary_logloss: 0.414457\ttraining's auc: 0.829583\n",
      "[179]\ttraining's binary_logloss: 0.414415\ttraining's auc: 0.829642\n",
      "[180]\ttraining's binary_logloss: 0.414365\ttraining's auc: 0.829707\n",
      "[181]\ttraining's binary_logloss: 0.414317\ttraining's auc: 0.829725\n",
      "[182]\ttraining's binary_logloss: 0.414256\ttraining's auc: 0.829772\n",
      "[183]\ttraining's binary_logloss: 0.414198\ttraining's auc: 0.829821\n",
      "[184]\ttraining's binary_logloss: 0.414156\ttraining's auc: 0.829857\n",
      "[185]\ttraining's binary_logloss: 0.414103\ttraining's auc: 0.829895\n",
      "[186]\ttraining's binary_logloss: 0.414049\ttraining's auc: 0.829958\n",
      "[187]\ttraining's binary_logloss: 0.414011\ttraining's auc: 0.829979\n",
      "[188]\ttraining's binary_logloss: 0.413965\ttraining's auc: 0.830044\n",
      "[189]\ttraining's binary_logloss: 0.413925\ttraining's auc: 0.830093\n",
      "[190]\ttraining's binary_logloss: 0.413865\ttraining's auc: 0.830153\n",
      "[191]\ttraining's binary_logloss: 0.413818\ttraining's auc: 0.83019\n",
      "[192]\ttraining's binary_logloss: 0.41377\ttraining's auc: 0.830249\n",
      "[193]\ttraining's binary_logloss: 0.413728\ttraining's auc: 0.830287\n",
      "[194]\ttraining's binary_logloss: 0.413671\ttraining's auc: 0.830373\n",
      "[195]\ttraining's binary_logloss: 0.413629\ttraining's auc: 0.830424\n",
      "[196]\ttraining's binary_logloss: 0.413583\ttraining's auc: 0.830466\n",
      "[197]\ttraining's binary_logloss: 0.413534\ttraining's auc: 0.830509\n",
      "[198]\ttraining's binary_logloss: 0.413486\ttraining's auc: 0.830545\n",
      "[199]\ttraining's binary_logloss: 0.413442\ttraining's auc: 0.830596\n",
      "[200]\ttraining's binary_logloss: 0.413389\ttraining's auc: 0.830672\n",
      "[201]\ttraining's binary_logloss: 0.413367\ttraining's auc: 0.830699\n",
      "[202]\ttraining's binary_logloss: 0.4133\ttraining's auc: 0.830732\n",
      "[203]\ttraining's binary_logloss: 0.413256\ttraining's auc: 0.83076\n",
      "[204]\ttraining's binary_logloss: 0.413216\ttraining's auc: 0.83079\n",
      "[205]\ttraining's binary_logloss: 0.413175\ttraining's auc: 0.83083\n",
      "[206]\ttraining's binary_logloss: 0.41314\ttraining's auc: 0.830858\n",
      "[207]\ttraining's binary_logloss: 0.413082\ttraining's auc: 0.830925\n",
      "[208]\ttraining's binary_logloss: 0.413025\ttraining's auc: 0.830991\n",
      "[209]\ttraining's binary_logloss: 0.412978\ttraining's auc: 0.831039\n",
      "[210]\ttraining's binary_logloss: 0.412938\ttraining's auc: 0.831084\n",
      "[211]\ttraining's binary_logloss: 0.412886\ttraining's auc: 0.831141\n",
      "[212]\ttraining's binary_logloss: 0.412839\ttraining's auc: 0.83123\n",
      "[213]\ttraining's binary_logloss: 0.412775\ttraining's auc: 0.83129\n",
      "[214]\ttraining's binary_logloss: 0.41271\ttraining's auc: 0.831357\n",
      "[215]\ttraining's binary_logloss: 0.412685\ttraining's auc: 0.831371\n",
      "[216]\ttraining's binary_logloss: 0.412638\ttraining's auc: 0.831417\n",
      "[217]\ttraining's binary_logloss: 0.412588\ttraining's auc: 0.831448\n",
      "[218]\ttraining's binary_logloss: 0.41253\ttraining's auc: 0.831474\n",
      "[219]\ttraining's binary_logloss: 0.412471\ttraining's auc: 0.831535\n",
      "[220]\ttraining's binary_logloss: 0.41243\ttraining's auc: 0.831584\n",
      "[221]\ttraining's binary_logloss: 0.412374\ttraining's auc: 0.831632\n",
      "[222]\ttraining's binary_logloss: 0.412332\ttraining's auc: 0.831674\n",
      "[223]\ttraining's binary_logloss: 0.412272\ttraining's auc: 0.831722\n",
      "[224]\ttraining's binary_logloss: 0.412228\ttraining's auc: 0.831783\n",
      "[225]\ttraining's binary_logloss: 0.412178\ttraining's auc: 0.83182\n",
      "[226]\ttraining's binary_logloss: 0.412105\ttraining's auc: 0.83184\n",
      "[227]\ttraining's binary_logloss: 0.412055\ttraining's auc: 0.831893\n",
      "[228]\ttraining's binary_logloss: 0.412017\ttraining's auc: 0.831926\n",
      "[229]\ttraining's binary_logloss: 0.411971\ttraining's auc: 0.832014\n",
      "[230]\ttraining's binary_logloss: 0.41192\ttraining's auc: 0.832102\n",
      "[231]\ttraining's binary_logloss: 0.411874\ttraining's auc: 0.832126\n",
      "[232]\ttraining's binary_logloss: 0.411841\ttraining's auc: 0.832166\n",
      "[233]\ttraining's binary_logloss: 0.411778\ttraining's auc: 0.832228\n",
      "[234]\ttraining's binary_logloss: 0.411755\ttraining's auc: 0.832239\n",
      "[235]\ttraining's binary_logloss: 0.411714\ttraining's auc: 0.832271\n",
      "[236]\ttraining's binary_logloss: 0.41168\ttraining's auc: 0.832311\n",
      "[237]\ttraining's binary_logloss: 0.41162\ttraining's auc: 0.832387\n",
      "[238]\ttraining's binary_logloss: 0.411562\ttraining's auc: 0.832438\n",
      "[239]\ttraining's binary_logloss: 0.411506\ttraining's auc: 0.832471\n",
      "[240]\ttraining's binary_logloss: 0.411471\ttraining's auc: 0.832496\n",
      "[241]\ttraining's binary_logloss: 0.411427\ttraining's auc: 0.832528\n",
      "[242]\ttraining's binary_logloss: 0.411376\ttraining's auc: 0.832611\n",
      "[243]\ttraining's binary_logloss: 0.411339\ttraining's auc: 0.832629\n",
      "[244]\ttraining's binary_logloss: 0.411289\ttraining's auc: 0.832668\n",
      "[245]\ttraining's binary_logloss: 0.411228\ttraining's auc: 0.832731\n",
      "[246]\ttraining's binary_logloss: 0.411184\ttraining's auc: 0.832747\n",
      "[247]\ttraining's binary_logloss: 0.411134\ttraining's auc: 0.832796\n",
      "[248]\ttraining's binary_logloss: 0.411074\ttraining's auc: 0.832843\n",
      "[249]\ttraining's binary_logloss: 0.411032\ttraining's auc: 0.832924\n",
      "[250]\ttraining's binary_logloss: 0.41099\ttraining's auc: 0.832966\n",
      "[251]\ttraining's binary_logloss: 0.410931\ttraining's auc: 0.83305\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[252]\ttraining's binary_logloss: 0.410879\ttraining's auc: 0.833077\n",
      "[253]\ttraining's binary_logloss: 0.410829\ttraining's auc: 0.833142\n",
      "[254]\ttraining's binary_logloss: 0.410796\ttraining's auc: 0.833175\n",
      "[255]\ttraining's binary_logloss: 0.410749\ttraining's auc: 0.833231\n",
      "[256]\ttraining's binary_logloss: 0.41072\ttraining's auc: 0.833247\n",
      "[257]\ttraining's binary_logloss: 0.410662\ttraining's auc: 0.833285\n",
      "[258]\ttraining's binary_logloss: 0.410615\ttraining's auc: 0.833305\n",
      "[259]\ttraining's binary_logloss: 0.410596\ttraining's auc: 0.833321\n",
      "[260]\ttraining's binary_logloss: 0.410531\ttraining's auc: 0.833345\n",
      "[261]\ttraining's binary_logloss: 0.410483\ttraining's auc: 0.833399\n",
      "[262]\ttraining's binary_logloss: 0.410424\ttraining's auc: 0.833467\n",
      "[263]\ttraining's binary_logloss: 0.410387\ttraining's auc: 0.833481\n",
      "[264]\ttraining's binary_logloss: 0.410338\ttraining's auc: 0.833527\n",
      "[265]\ttraining's binary_logloss: 0.410283\ttraining's auc: 0.833627\n",
      "[266]\ttraining's binary_logloss: 0.410227\ttraining's auc: 0.833687\n",
      "[267]\ttraining's binary_logloss: 0.410172\ttraining's auc: 0.833724\n",
      "[268]\ttraining's binary_logloss: 0.410131\ttraining's auc: 0.83377\n",
      "[269]\ttraining's binary_logloss: 0.41009\ttraining's auc: 0.833823\n",
      "[270]\ttraining's binary_logloss: 0.410041\ttraining's auc: 0.833863\n",
      "[271]\ttraining's binary_logloss: 0.409996\ttraining's auc: 0.833916\n",
      "[272]\ttraining's binary_logloss: 0.409939\ttraining's auc: 0.833974\n",
      "[273]\ttraining's binary_logloss: 0.409914\ttraining's auc: 0.833987\n",
      "[274]\ttraining's binary_logloss: 0.409879\ttraining's auc: 0.834032\n",
      "[275]\ttraining's binary_logloss: 0.409844\ttraining's auc: 0.83406\n",
      "[276]\ttraining's binary_logloss: 0.409794\ttraining's auc: 0.834081\n",
      "[277]\ttraining's binary_logloss: 0.40976\ttraining's auc: 0.83412\n",
      "[278]\ttraining's binary_logloss: 0.409708\ttraining's auc: 0.834145\n",
      "[279]\ttraining's binary_logloss: 0.409675\ttraining's auc: 0.834159\n",
      "[280]\ttraining's binary_logloss: 0.409629\ttraining's auc: 0.834202\n",
      "[281]\ttraining's binary_logloss: 0.409584\ttraining's auc: 0.834219\n",
      "[282]\ttraining's binary_logloss: 0.409541\ttraining's auc: 0.834268\n",
      "[283]\ttraining's binary_logloss: 0.409497\ttraining's auc: 0.834318\n",
      "[284]\ttraining's binary_logloss: 0.40945\ttraining's auc: 0.834335\n",
      "[285]\ttraining's binary_logloss: 0.409393\ttraining's auc: 0.834377\n",
      "[286]\ttraining's binary_logloss: 0.409331\ttraining's auc: 0.834447\n",
      "[287]\ttraining's binary_logloss: 0.409281\ttraining's auc: 0.834508\n",
      "[288]\ttraining's binary_logloss: 0.409231\ttraining's auc: 0.834558\n",
      "[289]\ttraining's binary_logloss: 0.409176\ttraining's auc: 0.834617\n",
      "[290]\ttraining's binary_logloss: 0.409115\ttraining's auc: 0.834642\n",
      "[291]\ttraining's binary_logloss: 0.409088\ttraining's auc: 0.834652\n",
      "[292]\ttraining's binary_logloss: 0.409028\ttraining's auc: 0.834706\n",
      "[293]\ttraining's binary_logloss: 0.408973\ttraining's auc: 0.834735\n",
      "[294]\ttraining's binary_logloss: 0.408928\ttraining's auc: 0.834753\n",
      "[295]\ttraining's binary_logloss: 0.408902\ttraining's auc: 0.834771\n",
      "[296]\ttraining's binary_logloss: 0.408867\ttraining's auc: 0.834801\n",
      "[297]\ttraining's binary_logloss: 0.408815\ttraining's auc: 0.834899\n",
      "[298]\ttraining's binary_logloss: 0.408768\ttraining's auc: 0.83492\n",
      "[299]\ttraining's binary_logloss: 0.408718\ttraining's auc: 0.834982\n",
      "[300]\ttraining's binary_logloss: 0.408682\ttraining's auc: 0.835032\n",
      "[301]\ttraining's binary_logloss: 0.408628\ttraining's auc: 0.835095\n",
      "[302]\ttraining's binary_logloss: 0.408582\ttraining's auc: 0.835124\n",
      "[303]\ttraining's binary_logloss: 0.40853\ttraining's auc: 0.8352\n",
      "[304]\ttraining's binary_logloss: 0.408481\ttraining's auc: 0.835252\n",
      "[305]\ttraining's binary_logloss: 0.408425\ttraining's auc: 0.835294\n",
      "[306]\ttraining's binary_logloss: 0.40837\ttraining's auc: 0.835349\n",
      "[307]\ttraining's binary_logloss: 0.408324\ttraining's auc: 0.835394\n",
      "[308]\ttraining's binary_logloss: 0.408265\ttraining's auc: 0.835448\n",
      "[309]\ttraining's binary_logloss: 0.40821\ttraining's auc: 0.835512\n",
      "[310]\ttraining's binary_logloss: 0.408156\ttraining's auc: 0.835575\n",
      "[311]\ttraining's binary_logloss: 0.408111\ttraining's auc: 0.835604\n",
      "[312]\ttraining's binary_logloss: 0.408057\ttraining's auc: 0.835649\n",
      "[313]\ttraining's binary_logloss: 0.408021\ttraining's auc: 0.835685\n",
      "[314]\ttraining's binary_logloss: 0.408003\ttraining's auc: 0.835701\n",
      "[315]\ttraining's binary_logloss: 0.407964\ttraining's auc: 0.835709\n",
      "[316]\ttraining's binary_logloss: 0.407915\ttraining's auc: 0.835726\n",
      "[317]\ttraining's binary_logloss: 0.40788\ttraining's auc: 0.835746\n",
      "[318]\ttraining's binary_logloss: 0.40784\ttraining's auc: 0.835774\n",
      "[319]\ttraining's binary_logloss: 0.407781\ttraining's auc: 0.835811\n",
      "[320]\ttraining's binary_logloss: 0.407737\ttraining's auc: 0.835898\n",
      "[321]\ttraining's binary_logloss: 0.407683\ttraining's auc: 0.835951\n",
      "[322]\ttraining's binary_logloss: 0.40764\ttraining's auc: 0.83597\n",
      "[323]\ttraining's binary_logloss: 0.407602\ttraining's auc: 0.835981\n",
      "[324]\ttraining's binary_logloss: 0.407551\ttraining's auc: 0.83604\n",
      "[325]\ttraining's binary_logloss: 0.407519\ttraining's auc: 0.836059\n",
      "[326]\ttraining's binary_logloss: 0.407486\ttraining's auc: 0.8361\n",
      "[327]\ttraining's binary_logloss: 0.407435\ttraining's auc: 0.836169\n",
      "[328]\ttraining's binary_logloss: 0.407383\ttraining's auc: 0.836226\n",
      "[329]\ttraining's binary_logloss: 0.407338\ttraining's auc: 0.83626\n",
      "[330]\ttraining's binary_logloss: 0.407311\ttraining's auc: 0.836277\n",
      "[331]\ttraining's binary_logloss: 0.407272\ttraining's auc: 0.836296\n",
      "[332]\ttraining's binary_logloss: 0.407244\ttraining's auc: 0.836319\n",
      "[333]\ttraining's binary_logloss: 0.407206\ttraining's auc: 0.836339\n",
      "[334]\ttraining's binary_logloss: 0.407167\ttraining's auc: 0.836354\n",
      "[335]\ttraining's binary_logloss: 0.40713\ttraining's auc: 0.836381\n",
      "[336]\ttraining's binary_logloss: 0.407085\ttraining's auc: 0.836426\n",
      "[337]\ttraining's binary_logloss: 0.407028\ttraining's auc: 0.836483\n",
      "[338]\ttraining's binary_logloss: 0.406965\ttraining's auc: 0.836548\n",
      "[339]\ttraining's binary_logloss: 0.406926\ttraining's auc: 0.8366\n",
      "[340]\ttraining's binary_logloss: 0.406888\ttraining's auc: 0.836631\n",
      "[341]\ttraining's binary_logloss: 0.40684\ttraining's auc: 0.836683\n",
      "[342]\ttraining's binary_logloss: 0.406787\ttraining's auc: 0.836735\n",
      "[343]\ttraining's binary_logloss: 0.406739\ttraining's auc: 0.836781\n",
      "[344]\ttraining's binary_logloss: 0.406703\ttraining's auc: 0.836845\n",
      "[345]\ttraining's binary_logloss: 0.406648\ttraining's auc: 0.836907\n",
      "[346]\ttraining's binary_logloss: 0.406616\ttraining's auc: 0.836948\n",
      "[347]\ttraining's binary_logloss: 0.406578\ttraining's auc: 0.836993\n",
      "[348]\ttraining's binary_logloss: 0.406531\ttraining's auc: 0.837041\n",
      "[349]\ttraining's binary_logloss: 0.406516\ttraining's auc: 0.837055\n",
      "[350]\ttraining's binary_logloss: 0.406473\ttraining's auc: 0.837072\n",
      "[351]\ttraining's binary_logloss: 0.40644\ttraining's auc: 0.837084\n",
      "[352]\ttraining's binary_logloss: 0.406395\ttraining's auc: 0.83711\n",
      "[353]\ttraining's binary_logloss: 0.406363\ttraining's auc: 0.83715\n",
      "[354]\ttraining's binary_logloss: 0.406348\ttraining's auc: 0.83716\n",
      "[355]\ttraining's binary_logloss: 0.4063\ttraining's auc: 0.837222\n",
      "[356]\ttraining's binary_logloss: 0.40625\ttraining's auc: 0.837262\n",
      "[357]\ttraining's binary_logloss: 0.406201\ttraining's auc: 0.837353\n",
      "[358]\ttraining's binary_logloss: 0.406158\ttraining's auc: 0.837403\n",
      "[359]\ttraining's binary_logloss: 0.406092\ttraining's auc: 0.837464\n",
      "[360]\ttraining's binary_logloss: 0.406059\ttraining's auc: 0.837494\n",
      "[361]\ttraining's binary_logloss: 0.405996\ttraining's auc: 0.83756\n",
      "[362]\ttraining's binary_logloss: 0.405943\ttraining's auc: 0.837652\n",
      "[363]\ttraining's binary_logloss: 0.405907\ttraining's auc: 0.837666\n",
      "[364]\ttraining's binary_logloss: 0.405851\ttraining's auc: 0.837709\n",
      "[365]\ttraining's binary_logloss: 0.405804\ttraining's auc: 0.837771\n",
      "[366]\ttraining's binary_logloss: 0.405756\ttraining's auc: 0.837809\n",
      "[367]\ttraining's binary_logloss: 0.4057\ttraining's auc: 0.837851\n",
      "[368]\ttraining's binary_logloss: 0.405661\ttraining's auc: 0.837897\n",
      "[369]\ttraining's binary_logloss: 0.405618\ttraining's auc: 0.837947\n",
      "[370]\ttraining's binary_logloss: 0.405566\ttraining's auc: 0.838\n",
      "[371]\ttraining's binary_logloss: 0.405551\ttraining's auc: 0.83801\n",
      "[372]\ttraining's binary_logloss: 0.405515\ttraining's auc: 0.83805\n",
      "[373]\ttraining's binary_logloss: 0.405478\ttraining's auc: 0.838098\n",
      "[374]\ttraining's binary_logloss: 0.405445\ttraining's auc: 0.83812\n",
      "[375]\ttraining's binary_logloss: 0.405406\ttraining's auc: 0.838147\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[376]\ttraining's binary_logloss: 0.405368\ttraining's auc: 0.838194\n",
      "[377]\ttraining's binary_logloss: 0.405316\ttraining's auc: 0.838265\n",
      "[378]\ttraining's binary_logloss: 0.405271\ttraining's auc: 0.838328\n",
      "[379]\ttraining's binary_logloss: 0.405229\ttraining's auc: 0.838392\n",
      "[380]\ttraining's binary_logloss: 0.405196\ttraining's auc: 0.838439\n",
      "[381]\ttraining's binary_logloss: 0.405158\ttraining's auc: 0.838459\n",
      "[382]\ttraining's binary_logloss: 0.405099\ttraining's auc: 0.838511\n",
      "[383]\ttraining's binary_logloss: 0.40505\ttraining's auc: 0.838563\n",
      "[384]\ttraining's binary_logloss: 0.404997\ttraining's auc: 0.838613\n",
      "[385]\ttraining's binary_logloss: 0.404946\ttraining's auc: 0.838667\n",
      "[386]\ttraining's binary_logloss: 0.404889\ttraining's auc: 0.838686\n",
      "[387]\ttraining's binary_logloss: 0.404837\ttraining's auc: 0.838744\n",
      "[388]\ttraining's binary_logloss: 0.4048\ttraining's auc: 0.838762\n",
      "[389]\ttraining's binary_logloss: 0.404764\ttraining's auc: 0.838798\n",
      "[390]\ttraining's binary_logloss: 0.404711\ttraining's auc: 0.838852\n",
      "[391]\ttraining's binary_logloss: 0.404651\ttraining's auc: 0.838888\n",
      "[392]\ttraining's binary_logloss: 0.4046\ttraining's auc: 0.838938\n",
      "[393]\ttraining's binary_logloss: 0.404542\ttraining's auc: 0.83899\n",
      "[394]\ttraining's binary_logloss: 0.404497\ttraining's auc: 0.839021\n",
      "[395]\ttraining's binary_logloss: 0.40446\ttraining's auc: 0.839056\n",
      "[396]\ttraining's binary_logloss: 0.404406\ttraining's auc: 0.8391\n",
      "[397]\ttraining's binary_logloss: 0.404378\ttraining's auc: 0.839138\n",
      "[398]\ttraining's binary_logloss: 0.404342\ttraining's auc: 0.839159\n",
      "[399]\ttraining's binary_logloss: 0.404303\ttraining's auc: 0.839203\n",
      "[400]\ttraining's binary_logloss: 0.404268\ttraining's auc: 0.839245\n",
      "[401]\ttraining's binary_logloss: 0.404247\ttraining's auc: 0.83926\n",
      "[402]\ttraining's binary_logloss: 0.404214\ttraining's auc: 0.839289\n",
      "[403]\ttraining's binary_logloss: 0.404165\ttraining's auc: 0.839325\n",
      "[404]\ttraining's binary_logloss: 0.404118\ttraining's auc: 0.839373\n",
      "[405]\ttraining's binary_logloss: 0.404057\ttraining's auc: 0.839431\n",
      "[406]\ttraining's binary_logloss: 0.404022\ttraining's auc: 0.839459\n",
      "[407]\ttraining's binary_logloss: 0.403993\ttraining's auc: 0.839478\n",
      "[408]\ttraining's binary_logloss: 0.403938\ttraining's auc: 0.839519\n",
      "[409]\ttraining's binary_logloss: 0.403901\ttraining's auc: 0.83954\n",
      "[410]\ttraining's binary_logloss: 0.403855\ttraining's auc: 0.839585\n",
      "[411]\ttraining's binary_logloss: 0.403791\ttraining's auc: 0.839633\n",
      "[412]\ttraining's binary_logloss: 0.403764\ttraining's auc: 0.839671\n",
      "[413]\ttraining's binary_logloss: 0.40373\ttraining's auc: 0.839683\n",
      "[414]\ttraining's binary_logloss: 0.403685\ttraining's auc: 0.839727\n",
      "[415]\ttraining's binary_logloss: 0.403661\ttraining's auc: 0.839737\n",
      "[416]\ttraining's binary_logloss: 0.40362\ttraining's auc: 0.839782\n",
      "[417]\ttraining's binary_logloss: 0.403589\ttraining's auc: 0.839795\n",
      "[418]\ttraining's binary_logloss: 0.403557\ttraining's auc: 0.839834\n",
      "[419]\ttraining's binary_logloss: 0.403509\ttraining's auc: 0.839894\n",
      "[420]\ttraining's binary_logloss: 0.403462\ttraining's auc: 0.839946\n",
      "[421]\ttraining's binary_logloss: 0.403422\ttraining's auc: 0.839971\n",
      "[422]\ttraining's binary_logloss: 0.403358\ttraining's auc: 0.84004\n",
      "[423]\ttraining's binary_logloss: 0.403323\ttraining's auc: 0.840075\n",
      "[424]\ttraining's binary_logloss: 0.403292\ttraining's auc: 0.840087\n",
      "[425]\ttraining's binary_logloss: 0.403258\ttraining's auc: 0.840117\n",
      "[426]\ttraining's binary_logloss: 0.403222\ttraining's auc: 0.84016\n",
      "[427]\ttraining's binary_logloss: 0.403208\ttraining's auc: 0.840174\n",
      "[428]\ttraining's binary_logloss: 0.403164\ttraining's auc: 0.840223\n",
      "[429]\ttraining's binary_logloss: 0.40315\ttraining's auc: 0.840231\n",
      "[430]\ttraining's binary_logloss: 0.403103\ttraining's auc: 0.840282\n",
      "[431]\ttraining's binary_logloss: 0.40307\ttraining's auc: 0.840293\n",
      "[432]\ttraining's binary_logloss: 0.40304\ttraining's auc: 0.840333\n",
      "[433]\ttraining's binary_logloss: 0.402979\ttraining's auc: 0.8404\n",
      "[434]\ttraining's binary_logloss: 0.402925\ttraining's auc: 0.840457\n",
      "[435]\ttraining's binary_logloss: 0.402878\ttraining's auc: 0.840497\n",
      "[436]\ttraining's binary_logloss: 0.402829\ttraining's auc: 0.840563\n",
      "[437]\ttraining's binary_logloss: 0.402806\ttraining's auc: 0.840574\n",
      "[438]\ttraining's binary_logloss: 0.402767\ttraining's auc: 0.840595\n",
      "[439]\ttraining's binary_logloss: 0.402719\ttraining's auc: 0.840657\n",
      "[440]\ttraining's binary_logloss: 0.402682\ttraining's auc: 0.840669\n",
      "[441]\ttraining's binary_logloss: 0.402627\ttraining's auc: 0.840691\n",
      "[442]\ttraining's binary_logloss: 0.402572\ttraining's auc: 0.84075\n",
      "[443]\ttraining's binary_logloss: 0.402517\ttraining's auc: 0.8408\n",
      "[444]\ttraining's binary_logloss: 0.402477\ttraining's auc: 0.840817\n",
      "[445]\ttraining's binary_logloss: 0.402423\ttraining's auc: 0.840874\n",
      "[446]\ttraining's binary_logloss: 0.402389\ttraining's auc: 0.840911\n",
      "[447]\ttraining's binary_logloss: 0.402341\ttraining's auc: 0.840991\n",
      "[448]\ttraining's binary_logloss: 0.402293\ttraining's auc: 0.841021\n",
      "[449]\ttraining's binary_logloss: 0.402248\ttraining's auc: 0.841078\n",
      "[450]\ttraining's binary_logloss: 0.402213\ttraining's auc: 0.841116\n",
      "[451]\ttraining's binary_logloss: 0.402188\ttraining's auc: 0.841129\n",
      "[452]\ttraining's binary_logloss: 0.402155\ttraining's auc: 0.841163\n",
      "[453]\ttraining's binary_logloss: 0.402118\ttraining's auc: 0.841179\n",
      "[454]\ttraining's binary_logloss: 0.40209\ttraining's auc: 0.841213\n",
      "[455]\ttraining's binary_logloss: 0.402042\ttraining's auc: 0.841254\n",
      "[456]\ttraining's binary_logloss: 0.401999\ttraining's auc: 0.841338\n",
      "[457]\ttraining's binary_logloss: 0.401965\ttraining's auc: 0.841351\n",
      "[458]\ttraining's binary_logloss: 0.401905\ttraining's auc: 0.841374\n",
      "[459]\ttraining's binary_logloss: 0.401853\ttraining's auc: 0.841449\n",
      "[460]\ttraining's binary_logloss: 0.401797\ttraining's auc: 0.84147\n",
      "[461]\ttraining's binary_logloss: 0.40175\ttraining's auc: 0.841489\n",
      "[462]\ttraining's binary_logloss: 0.401716\ttraining's auc: 0.841533\n",
      "[463]\ttraining's binary_logloss: 0.40166\ttraining's auc: 0.841577\n",
      "[464]\ttraining's binary_logloss: 0.40161\ttraining's auc: 0.841596\n",
      "[465]\ttraining's binary_logloss: 0.401592\ttraining's auc: 0.841608\n",
      "[466]\ttraining's binary_logloss: 0.40155\ttraining's auc: 0.841628\n",
      "[467]\ttraining's binary_logloss: 0.4015\ttraining's auc: 0.841716\n",
      "[468]\ttraining's binary_logloss: 0.401449\ttraining's auc: 0.841774\n",
      "[469]\ttraining's binary_logloss: 0.401407\ttraining's auc: 0.84182\n",
      "[470]\ttraining's binary_logloss: 0.401359\ttraining's auc: 0.841873\n",
      "[471]\ttraining's binary_logloss: 0.401303\ttraining's auc: 0.841914\n",
      "[472]\ttraining's binary_logloss: 0.401296\ttraining's auc: 0.84192\n",
      "[473]\ttraining's binary_logloss: 0.401268\ttraining's auc: 0.841941\n",
      "[474]\ttraining's binary_logloss: 0.401237\ttraining's auc: 0.841952\n",
      "[475]\ttraining's binary_logloss: 0.40119\ttraining's auc: 0.841991\n",
      "[476]\ttraining's binary_logloss: 0.401152\ttraining's auc: 0.842033\n",
      "[477]\ttraining's binary_logloss: 0.40111\ttraining's auc: 0.842087\n",
      "[478]\ttraining's binary_logloss: 0.401079\ttraining's auc: 0.842123\n",
      "[479]\ttraining's binary_logloss: 0.401067\ttraining's auc: 0.842133\n",
      "[480]\ttraining's binary_logloss: 0.401022\ttraining's auc: 0.842186\n",
      "[481]\ttraining's binary_logloss: 0.400989\ttraining's auc: 0.842232\n",
      "[482]\ttraining's binary_logloss: 0.400948\ttraining's auc: 0.842275\n",
      "[483]\ttraining's binary_logloss: 0.400896\ttraining's auc: 0.842343\n",
      "[484]\ttraining's binary_logloss: 0.400862\ttraining's auc: 0.842367\n",
      "[485]\ttraining's binary_logloss: 0.400833\ttraining's auc: 0.842402\n",
      "[486]\ttraining's binary_logloss: 0.400791\ttraining's auc: 0.842418\n",
      "[487]\ttraining's binary_logloss: 0.400752\ttraining's auc: 0.842436\n",
      "[488]\ttraining's binary_logloss: 0.400713\ttraining's auc: 0.842491\n",
      "[489]\ttraining's binary_logloss: 0.400668\ttraining's auc: 0.842537\n",
      "[490]\ttraining's binary_logloss: 0.40063\ttraining's auc: 0.842564\n",
      "[491]\ttraining's binary_logloss: 0.400597\ttraining's auc: 0.842577\n",
      "[492]\ttraining's binary_logloss: 0.400557\ttraining's auc: 0.842649\n",
      "[493]\ttraining's binary_logloss: 0.400501\ttraining's auc: 0.842699\n",
      "[494]\ttraining's binary_logloss: 0.400444\ttraining's auc: 0.842767\n",
      "[495]\ttraining's binary_logloss: 0.400411\ttraining's auc: 0.842785\n",
      "[496]\ttraining's binary_logloss: 0.40038\ttraining's auc: 0.842793\n",
      "[497]\ttraining's binary_logloss: 0.400351\ttraining's auc: 0.842847\n",
      "[498]\ttraining's binary_logloss: 0.400319\ttraining's auc: 0.84289\n",
      "[499]\ttraining's binary_logloss: 0.400283\ttraining's auc: 0.842905\n",
      "[500]\ttraining's binary_logloss: 0.40023\ttraining's auc: 0.842942\n",
      "[501]\ttraining's binary_logloss: 0.4002\ttraining's auc: 0.842975\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[502]\ttraining's binary_logloss: 0.400168\ttraining's auc: 0.843013\n",
      "[503]\ttraining's binary_logloss: 0.400146\ttraining's auc: 0.843029\n",
      "[504]\ttraining's binary_logloss: 0.400093\ttraining's auc: 0.843116\n",
      "[505]\ttraining's binary_logloss: 0.400037\ttraining's auc: 0.843157\n",
      "[506]\ttraining's binary_logloss: 0.400003\ttraining's auc: 0.843203\n",
      "[507]\ttraining's binary_logloss: 0.399952\ttraining's auc: 0.843237\n",
      "[508]\ttraining's binary_logloss: 0.399912\ttraining's auc: 0.843297\n",
      "[509]\ttraining's binary_logloss: 0.399867\ttraining's auc: 0.843382\n",
      "[510]\ttraining's binary_logloss: 0.399805\ttraining's auc: 0.843455\n",
      "[511]\ttraining's binary_logloss: 0.399752\ttraining's auc: 0.843541\n",
      "[512]\ttraining's binary_logloss: 0.399704\ttraining's auc: 0.843567\n",
      "[513]\ttraining's binary_logloss: 0.399664\ttraining's auc: 0.843587\n",
      "[514]\ttraining's binary_logloss: 0.399625\ttraining's auc: 0.843601\n",
      "[515]\ttraining's binary_logloss: 0.399567\ttraining's auc: 0.843651\n",
      "[516]\ttraining's binary_logloss: 0.399528\ttraining's auc: 0.843671\n",
      "[517]\ttraining's binary_logloss: 0.399501\ttraining's auc: 0.843682\n",
      "[518]\ttraining's binary_logloss: 0.399444\ttraining's auc: 0.843701\n",
      "[519]\ttraining's binary_logloss: 0.399387\ttraining's auc: 0.843733\n",
      "[520]\ttraining's binary_logloss: 0.399339\ttraining's auc: 0.843789\n",
      "[521]\ttraining's binary_logloss: 0.399285\ttraining's auc: 0.843846\n",
      "[522]\ttraining's binary_logloss: 0.399235\ttraining's auc: 0.843932\n",
      "[523]\ttraining's binary_logloss: 0.399198\ttraining's auc: 0.843966\n",
      "[524]\ttraining's binary_logloss: 0.399145\ttraining's auc: 0.843987\n",
      "[525]\ttraining's binary_logloss: 0.399081\ttraining's auc: 0.844087\n",
      "[526]\ttraining's binary_logloss: 0.399031\ttraining's auc: 0.844113\n",
      "[527]\ttraining's binary_logloss: 0.398979\ttraining's auc: 0.844176\n",
      "[528]\ttraining's binary_logloss: 0.398936\ttraining's auc: 0.844191\n",
      "[529]\ttraining's binary_logloss: 0.398889\ttraining's auc: 0.844268\n",
      "[530]\ttraining's binary_logloss: 0.398852\ttraining's auc: 0.844335\n",
      "[531]\ttraining's binary_logloss: 0.398821\ttraining's auc: 0.844369\n",
      "[532]\ttraining's binary_logloss: 0.398775\ttraining's auc: 0.844432\n",
      "[533]\ttraining's binary_logloss: 0.39872\ttraining's auc: 0.844496\n",
      "[534]\ttraining's binary_logloss: 0.398689\ttraining's auc: 0.844522\n",
      "[535]\ttraining's binary_logloss: 0.398642\ttraining's auc: 0.844582\n",
      "[536]\ttraining's binary_logloss: 0.398608\ttraining's auc: 0.844599\n",
      "[537]\ttraining's binary_logloss: 0.398566\ttraining's auc: 0.844675\n",
      "[538]\ttraining's binary_logloss: 0.398506\ttraining's auc: 0.844706\n",
      "[539]\ttraining's binary_logloss: 0.398465\ttraining's auc: 0.84474\n",
      "[540]\ttraining's binary_logloss: 0.398439\ttraining's auc: 0.844778\n",
      "[541]\ttraining's binary_logloss: 0.398402\ttraining's auc: 0.84482\n",
      "[542]\ttraining's binary_logloss: 0.398371\ttraining's auc: 0.844846\n",
      "[543]\ttraining's binary_logloss: 0.398324\ttraining's auc: 0.84487\n",
      "[544]\ttraining's binary_logloss: 0.398272\ttraining's auc: 0.84493\n",
      "[545]\ttraining's binary_logloss: 0.398228\ttraining's auc: 0.844973\n",
      "[546]\ttraining's binary_logloss: 0.398189\ttraining's auc: 0.845021\n",
      "[547]\ttraining's binary_logloss: 0.398157\ttraining's auc: 0.845039\n",
      "[548]\ttraining's binary_logloss: 0.398101\ttraining's auc: 0.845067\n",
      "[549]\ttraining's binary_logloss: 0.398048\ttraining's auc: 0.845133\n",
      "[550]\ttraining's binary_logloss: 0.398007\ttraining's auc: 0.845149\n",
      "[551]\ttraining's binary_logloss: 0.39795\ttraining's auc: 0.84521\n",
      "[552]\ttraining's binary_logloss: 0.397905\ttraining's auc: 0.845246\n",
      "[553]\ttraining's binary_logloss: 0.397851\ttraining's auc: 0.845307\n",
      "[554]\ttraining's binary_logloss: 0.397822\ttraining's auc: 0.845334\n",
      "[555]\ttraining's binary_logloss: 0.397798\ttraining's auc: 0.845349\n",
      "[556]\ttraining's binary_logloss: 0.397738\ttraining's auc: 0.845381\n",
      "[557]\ttraining's binary_logloss: 0.397712\ttraining's auc: 0.845407\n",
      "[558]\ttraining's binary_logloss: 0.397661\ttraining's auc: 0.845464\n",
      "[559]\ttraining's binary_logloss: 0.397619\ttraining's auc: 0.845484\n",
      "[560]\ttraining's binary_logloss: 0.397572\ttraining's auc: 0.845565\n",
      "[561]\ttraining's binary_logloss: 0.397544\ttraining's auc: 0.845578\n",
      "[562]\ttraining's binary_logloss: 0.397489\ttraining's auc: 0.845637\n",
      "[563]\ttraining's binary_logloss: 0.397435\ttraining's auc: 0.845692\n",
      "[564]\ttraining's binary_logloss: 0.397389\ttraining's auc: 0.84572\n",
      "[565]\ttraining's binary_logloss: 0.397358\ttraining's auc: 0.845734\n",
      "[566]\ttraining's binary_logloss: 0.397305\ttraining's auc: 0.845782\n",
      "[567]\ttraining's binary_logloss: 0.397272\ttraining's auc: 0.845802\n",
      "[568]\ttraining's binary_logloss: 0.397225\ttraining's auc: 0.845845\n",
      "[569]\ttraining's binary_logloss: 0.397174\ttraining's auc: 0.845866\n",
      "[570]\ttraining's binary_logloss: 0.397128\ttraining's auc: 0.845917\n",
      "[571]\ttraining's binary_logloss: 0.397091\ttraining's auc: 0.845954\n",
      "[572]\ttraining's binary_logloss: 0.397043\ttraining's auc: 0.846005\n",
      "[573]\ttraining's binary_logloss: 0.396995\ttraining's auc: 0.846057\n",
      "[574]\ttraining's binary_logloss: 0.396968\ttraining's auc: 0.846091\n",
      "[575]\ttraining's binary_logloss: 0.396914\ttraining's auc: 0.846139\n",
      "[576]\ttraining's binary_logloss: 0.396885\ttraining's auc: 0.846186\n",
      "[577]\ttraining's binary_logloss: 0.396851\ttraining's auc: 0.846217\n",
      "[578]\ttraining's binary_logloss: 0.396812\ttraining's auc: 0.846237\n",
      "[579]\ttraining's binary_logloss: 0.396784\ttraining's auc: 0.846257\n",
      "[580]\ttraining's binary_logloss: 0.396736\ttraining's auc: 0.846288\n",
      "[581]\ttraining's binary_logloss: 0.396689\ttraining's auc: 0.846309\n",
      "[582]\ttraining's binary_logloss: 0.396645\ttraining's auc: 0.846332\n",
      "[583]\ttraining's binary_logloss: 0.396618\ttraining's auc: 0.846343\n",
      "[584]\ttraining's binary_logloss: 0.39656\ttraining's auc: 0.846372\n",
      "[585]\ttraining's binary_logloss: 0.39653\ttraining's auc: 0.84641\n",
      "[586]\ttraining's binary_logloss: 0.396492\ttraining's auc: 0.846448\n",
      "[587]\ttraining's binary_logloss: 0.396442\ttraining's auc: 0.846513\n",
      "[588]\ttraining's binary_logloss: 0.396393\ttraining's auc: 0.846563\n",
      "[589]\ttraining's binary_logloss: 0.396348\ttraining's auc: 0.846605\n",
      "[590]\ttraining's binary_logloss: 0.396299\ttraining's auc: 0.846649\n",
      "[591]\ttraining's binary_logloss: 0.396247\ttraining's auc: 0.846693\n",
      "[592]\ttraining's binary_logloss: 0.396205\ttraining's auc: 0.846712\n",
      "[593]\ttraining's binary_logloss: 0.396156\ttraining's auc: 0.846751\n",
      "[594]\ttraining's binary_logloss: 0.396128\ttraining's auc: 0.846768\n",
      "[595]\ttraining's binary_logloss: 0.396091\ttraining's auc: 0.846807\n",
      "[596]\ttraining's binary_logloss: 0.396062\ttraining's auc: 0.846829\n",
      "[597]\ttraining's binary_logloss: 0.396022\ttraining's auc: 0.846878\n",
      "[598]\ttraining's binary_logloss: 0.395994\ttraining's auc: 0.846892\n",
      "[599]\ttraining's binary_logloss: 0.395951\ttraining's auc: 0.84692\n",
      "[600]\ttraining's binary_logloss: 0.395922\ttraining's auc: 0.846943\n",
      "[601]\ttraining's binary_logloss: 0.395874\ttraining's auc: 0.846997\n",
      "[602]\ttraining's binary_logloss: 0.395825\ttraining's auc: 0.847048\n",
      "[603]\ttraining's binary_logloss: 0.395792\ttraining's auc: 0.847059\n",
      "[604]\ttraining's binary_logloss: 0.39575\ttraining's auc: 0.847102\n",
      "[605]\ttraining's binary_logloss: 0.395699\ttraining's auc: 0.847139\n",
      "[606]\ttraining's binary_logloss: 0.395649\ttraining's auc: 0.847204\n",
      "[607]\ttraining's binary_logloss: 0.395597\ttraining's auc: 0.847224\n",
      "[608]\ttraining's binary_logloss: 0.395548\ttraining's auc: 0.847254\n",
      "[609]\ttraining's binary_logloss: 0.395522\ttraining's auc: 0.847284\n",
      "[610]\ttraining's binary_logloss: 0.395472\ttraining's auc: 0.847351\n",
      "[611]\ttraining's binary_logloss: 0.395434\ttraining's auc: 0.847394\n",
      "[612]\ttraining's binary_logloss: 0.395386\ttraining's auc: 0.847441\n",
      "[613]\ttraining's binary_logloss: 0.395339\ttraining's auc: 0.847457\n",
      "[614]\ttraining's binary_logloss: 0.395299\ttraining's auc: 0.8475\n",
      "[615]\ttraining's binary_logloss: 0.395246\ttraining's auc: 0.847521\n",
      "[616]\ttraining's binary_logloss: 0.395219\ttraining's auc: 0.847546\n",
      "[617]\ttraining's binary_logloss: 0.395184\ttraining's auc: 0.847564\n",
      "[618]\ttraining's binary_logloss: 0.395171\ttraining's auc: 0.847582\n",
      "[619]\ttraining's binary_logloss: 0.395148\ttraining's auc: 0.847612\n",
      "[620]\ttraining's binary_logloss: 0.395118\ttraining's auc: 0.847629\n",
      "[621]\ttraining's binary_logloss: 0.395063\ttraining's auc: 0.847701\n",
      "[622]\ttraining's binary_logloss: 0.395029\ttraining's auc: 0.847734\n",
      "[623]\ttraining's binary_logloss: 0.394991\ttraining's auc: 0.847753\n",
      "[624]\ttraining's binary_logloss: 0.394949\ttraining's auc: 0.847796\n",
      "[625]\ttraining's binary_logloss: 0.394914\ttraining's auc: 0.847823\n",
      "[626]\ttraining's binary_logloss: 0.394907\ttraining's auc: 0.847832\n",
      "[627]\ttraining's binary_logloss: 0.394862\ttraining's auc: 0.847895\n",
      "[628]\ttraining's binary_logloss: 0.394809\ttraining's auc: 0.847958\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[629]\ttraining's binary_logloss: 0.394756\ttraining's auc: 0.84801\n",
      "[630]\ttraining's binary_logloss: 0.39471\ttraining's auc: 0.848068\n",
      "[631]\ttraining's binary_logloss: 0.394668\ttraining's auc: 0.848131\n",
      "[632]\ttraining's binary_logloss: 0.394634\ttraining's auc: 0.848163\n",
      "[633]\ttraining's binary_logloss: 0.394588\ttraining's auc: 0.848219\n",
      "[634]\ttraining's binary_logloss: 0.394541\ttraining's auc: 0.848242\n",
      "[635]\ttraining's binary_logloss: 0.394485\ttraining's auc: 0.848289\n",
      "[636]\ttraining's binary_logloss: 0.394443\ttraining's auc: 0.84833\n",
      "[637]\ttraining's binary_logloss: 0.394404\ttraining's auc: 0.848379\n",
      "[638]\ttraining's binary_logloss: 0.394384\ttraining's auc: 0.848392\n",
      "[639]\ttraining's binary_logloss: 0.394352\ttraining's auc: 0.848409\n",
      "[640]\ttraining's binary_logloss: 0.394307\ttraining's auc: 0.848447\n",
      "[641]\ttraining's binary_logloss: 0.394261\ttraining's auc: 0.848471\n",
      "[642]\ttraining's binary_logloss: 0.394216\ttraining's auc: 0.848497\n",
      "[643]\ttraining's binary_logloss: 0.394191\ttraining's auc: 0.84852\n",
      "[644]\ttraining's binary_logloss: 0.394165\ttraining's auc: 0.848537\n",
      "[645]\ttraining's binary_logloss: 0.394122\ttraining's auc: 0.848581\n",
      "[646]\ttraining's binary_logloss: 0.394079\ttraining's auc: 0.848623\n",
      "[647]\ttraining's binary_logloss: 0.394034\ttraining's auc: 0.84867\n",
      "[648]\ttraining's binary_logloss: 0.39399\ttraining's auc: 0.84872\n",
      "[649]\ttraining's binary_logloss: 0.393941\ttraining's auc: 0.848771\n",
      "[650]\ttraining's binary_logloss: 0.393899\ttraining's auc: 0.848819\n",
      "[651]\ttraining's binary_logloss: 0.393882\ttraining's auc: 0.84883\n",
      "[652]\ttraining's binary_logloss: 0.393834\ttraining's auc: 0.848865\n",
      "[653]\ttraining's binary_logloss: 0.393792\ttraining's auc: 0.848884\n",
      "[654]\ttraining's binary_logloss: 0.393738\ttraining's auc: 0.848931\n",
      "[655]\ttraining's binary_logloss: 0.393686\ttraining's auc: 0.848962\n",
      "[656]\ttraining's binary_logloss: 0.393673\ttraining's auc: 0.84897\n",
      "[657]\ttraining's binary_logloss: 0.393623\ttraining's auc: 0.849046\n",
      "[658]\ttraining's binary_logloss: 0.393576\ttraining's auc: 0.849101\n",
      "[659]\ttraining's binary_logloss: 0.393572\ttraining's auc: 0.849104\n",
      "[660]\ttraining's binary_logloss: 0.393536\ttraining's auc: 0.849119\n",
      "[661]\ttraining's binary_logloss: 0.393492\ttraining's auc: 0.849173\n",
      "[662]\ttraining's binary_logloss: 0.393469\ttraining's auc: 0.849183\n",
      "[663]\ttraining's binary_logloss: 0.393435\ttraining's auc: 0.849196\n",
      "[664]\ttraining's binary_logloss: 0.393386\ttraining's auc: 0.849273\n",
      "[665]\ttraining's binary_logloss: 0.393342\ttraining's auc: 0.849313\n",
      "[666]\ttraining's binary_logloss: 0.393314\ttraining's auc: 0.849326\n",
      "[667]\ttraining's binary_logloss: 0.393276\ttraining's auc: 0.849349\n",
      "[668]\ttraining's binary_logloss: 0.393253\ttraining's auc: 0.849361\n",
      "[669]\ttraining's binary_logloss: 0.393228\ttraining's auc: 0.849377\n",
      "[670]\ttraining's binary_logloss: 0.3932\ttraining's auc: 0.84939\n",
      "[671]\ttraining's binary_logloss: 0.393172\ttraining's auc: 0.849426\n",
      "[672]\ttraining's binary_logloss: 0.393162\ttraining's auc: 0.849431\n",
      "[673]\ttraining's binary_logloss: 0.393112\ttraining's auc: 0.849468\n",
      "[674]\ttraining's binary_logloss: 0.393071\ttraining's auc: 0.849506\n",
      "[675]\ttraining's binary_logloss: 0.393018\ttraining's auc: 0.849542\n",
      "[676]\ttraining's binary_logloss: 0.392968\ttraining's auc: 0.849584\n",
      "[677]\ttraining's binary_logloss: 0.39293\ttraining's auc: 0.849628\n",
      "[678]\ttraining's binary_logloss: 0.392888\ttraining's auc: 0.849681\n",
      "[679]\ttraining's binary_logloss: 0.392852\ttraining's auc: 0.849698\n",
      "[680]\ttraining's binary_logloss: 0.392831\ttraining's auc: 0.849715\n",
      "[681]\ttraining's binary_logloss: 0.392795\ttraining's auc: 0.849747\n",
      "[682]\ttraining's binary_logloss: 0.392767\ttraining's auc: 0.849759\n",
      "[683]\ttraining's binary_logloss: 0.392724\ttraining's auc: 0.849797\n",
      "[684]\ttraining's binary_logloss: 0.392687\ttraining's auc: 0.849822\n",
      "[685]\ttraining's binary_logloss: 0.392645\ttraining's auc: 0.849846\n",
      "[686]\ttraining's binary_logloss: 0.392596\ttraining's auc: 0.849884\n",
      "[687]\ttraining's binary_logloss: 0.392544\ttraining's auc: 0.849906\n",
      "[688]\ttraining's binary_logloss: 0.3925\ttraining's auc: 0.849963\n",
      "[689]\ttraining's binary_logloss: 0.392448\ttraining's auc: 0.850019\n",
      "[690]\ttraining's binary_logloss: 0.392403\ttraining's auc: 0.850069\n",
      "[691]\ttraining's binary_logloss: 0.392348\ttraining's auc: 0.850114\n",
      "[692]\ttraining's binary_logloss: 0.392299\ttraining's auc: 0.850147\n",
      "[693]\ttraining's binary_logloss: 0.39225\ttraining's auc: 0.850196\n",
      "[694]\ttraining's binary_logloss: 0.392208\ttraining's auc: 0.850236\n",
      "[695]\ttraining's binary_logloss: 0.392174\ttraining's auc: 0.850252\n",
      "[696]\ttraining's binary_logloss: 0.392128\ttraining's auc: 0.850294\n",
      "[697]\ttraining's binary_logloss: 0.392077\ttraining's auc: 0.850365\n",
      "[698]\ttraining's binary_logloss: 0.39204\ttraining's auc: 0.850404\n",
      "[699]\ttraining's binary_logloss: 0.392011\ttraining's auc: 0.850414\n",
      "[700]\ttraining's binary_logloss: 0.39196\ttraining's auc: 0.850457\n",
      "[701]\ttraining's binary_logloss: 0.391921\ttraining's auc: 0.850498\n",
      "[702]\ttraining's binary_logloss: 0.391873\ttraining's auc: 0.850525\n",
      "[703]\ttraining's binary_logloss: 0.391835\ttraining's auc: 0.850546\n",
      "[704]\ttraining's binary_logloss: 0.391787\ttraining's auc: 0.850592\n",
      "[705]\ttraining's binary_logloss: 0.391748\ttraining's auc: 0.850611\n",
      "[706]\ttraining's binary_logloss: 0.391715\ttraining's auc: 0.850629\n",
      "[707]\ttraining's binary_logloss: 0.391679\ttraining's auc: 0.850647\n",
      "[708]\ttraining's binary_logloss: 0.391647\ttraining's auc: 0.850661\n",
      "[709]\ttraining's binary_logloss: 0.391595\ttraining's auc: 0.850712\n",
      "[710]\ttraining's binary_logloss: 0.391546\ttraining's auc: 0.850746\n",
      "[711]\ttraining's binary_logloss: 0.39152\ttraining's auc: 0.85077\n",
      "[712]\ttraining's binary_logloss: 0.39147\ttraining's auc: 0.850832\n",
      "[713]\ttraining's binary_logloss: 0.391428\ttraining's auc: 0.850895\n",
      "[714]\ttraining's binary_logloss: 0.391386\ttraining's auc: 0.85095\n",
      "[715]\ttraining's binary_logloss: 0.39135\ttraining's auc: 0.850984\n",
      "[716]\ttraining's binary_logloss: 0.391303\ttraining's auc: 0.851035\n",
      "[717]\ttraining's binary_logloss: 0.391287\ttraining's auc: 0.851046\n",
      "[718]\ttraining's binary_logloss: 0.391246\ttraining's auc: 0.851082\n",
      "[719]\ttraining's binary_logloss: 0.391199\ttraining's auc: 0.851118\n",
      "[720]\ttraining's binary_logloss: 0.391157\ttraining's auc: 0.851172\n",
      "[721]\ttraining's binary_logloss: 0.391107\ttraining's auc: 0.85123\n",
      "[722]\ttraining's binary_logloss: 0.391076\ttraining's auc: 0.851243\n",
      "[723]\ttraining's binary_logloss: 0.391034\ttraining's auc: 0.851309\n",
      "[724]\ttraining's binary_logloss: 0.390998\ttraining's auc: 0.851347\n",
      "[725]\ttraining's binary_logloss: 0.390947\ttraining's auc: 0.851393\n",
      "[726]\ttraining's binary_logloss: 0.390895\ttraining's auc: 0.851442\n",
      "[727]\ttraining's binary_logloss: 0.390851\ttraining's auc: 0.851493\n",
      "[728]\ttraining's binary_logloss: 0.390825\ttraining's auc: 0.851502\n",
      "[729]\ttraining's binary_logloss: 0.390779\ttraining's auc: 0.851527\n",
      "[730]\ttraining's binary_logloss: 0.390737\ttraining's auc: 0.851586\n",
      "[731]\ttraining's binary_logloss: 0.390693\ttraining's auc: 0.851615\n",
      "[732]\ttraining's binary_logloss: 0.390641\ttraining's auc: 0.851666\n",
      "[733]\ttraining's binary_logloss: 0.39061\ttraining's auc: 0.851689\n",
      "[734]\ttraining's binary_logloss: 0.390558\ttraining's auc: 0.851752\n",
      "[735]\ttraining's binary_logloss: 0.390521\ttraining's auc: 0.851795\n",
      "[736]\ttraining's binary_logloss: 0.390472\ttraining's auc: 0.85184\n",
      "[737]\ttraining's binary_logloss: 0.390425\ttraining's auc: 0.851882\n",
      "[738]\ttraining's binary_logloss: 0.39037\ttraining's auc: 0.851944\n",
      "[739]\ttraining's binary_logloss: 0.390334\ttraining's auc: 0.851965\n",
      "[740]\ttraining's binary_logloss: 0.390307\ttraining's auc: 0.851976\n",
      "[741]\ttraining's binary_logloss: 0.390273\ttraining's auc: 0.852015\n",
      "[742]\ttraining's binary_logloss: 0.390221\ttraining's auc: 0.852066\n",
      "[743]\ttraining's binary_logloss: 0.390174\ttraining's auc: 0.852088\n",
      "[744]\ttraining's binary_logloss: 0.390121\ttraining's auc: 0.85216\n",
      "[745]\ttraining's binary_logloss: 0.390074\ttraining's auc: 0.852207\n",
      "[746]\ttraining's binary_logloss: 0.390026\ttraining's auc: 0.852258\n",
      "[747]\ttraining's binary_logloss: 0.390001\ttraining's auc: 0.852289\n",
      "[748]\ttraining's binary_logloss: 0.389961\ttraining's auc: 0.852353\n",
      "[749]\ttraining's binary_logloss: 0.389924\ttraining's auc: 0.852369\n",
      "[750]\ttraining's binary_logloss: 0.3899\ttraining's auc: 0.852403\n",
      "[751]\ttraining's binary_logloss: 0.389855\ttraining's auc: 0.852444\n",
      "[752]\ttraining's binary_logloss: 0.389798\ttraining's auc: 0.85246\n",
      "[753]\ttraining's binary_logloss: 0.389761\ttraining's auc: 0.852498\n",
      "[754]\ttraining's binary_logloss: 0.389724\ttraining's auc: 0.85253\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[755]\ttraining's binary_logloss: 0.389689\ttraining's auc: 0.852548\n",
      "[756]\ttraining's binary_logloss: 0.389659\ttraining's auc: 0.85257\n",
      "[757]\ttraining's binary_logloss: 0.389635\ttraining's auc: 0.852578\n",
      "[758]\ttraining's binary_logloss: 0.389583\ttraining's auc: 0.852605\n",
      "[759]\ttraining's binary_logloss: 0.389554\ttraining's auc: 0.852621\n",
      "[760]\ttraining's binary_logloss: 0.38953\ttraining's auc: 0.852632\n",
      "[761]\ttraining's binary_logloss: 0.389488\ttraining's auc: 0.852674\n",
      "[762]\ttraining's binary_logloss: 0.389442\ttraining's auc: 0.852716\n",
      "[763]\ttraining's binary_logloss: 0.389401\ttraining's auc: 0.852767\n",
      "[764]\ttraining's binary_logloss: 0.389354\ttraining's auc: 0.852833\n",
      "[765]\ttraining's binary_logloss: 0.389309\ttraining's auc: 0.852882\n",
      "[766]\ttraining's binary_logloss: 0.389271\ttraining's auc: 0.852901\n",
      "[767]\ttraining's binary_logloss: 0.389231\ttraining's auc: 0.852936\n",
      "[768]\ttraining's binary_logloss: 0.38919\ttraining's auc: 0.85297\n",
      "[769]\ttraining's binary_logloss: 0.389151\ttraining's auc: 0.853005\n",
      "[770]\ttraining's binary_logloss: 0.389102\ttraining's auc: 0.85305\n",
      "[771]\ttraining's binary_logloss: 0.389067\ttraining's auc: 0.853098\n",
      "[772]\ttraining's binary_logloss: 0.389029\ttraining's auc: 0.853148\n",
      "[773]\ttraining's binary_logloss: 0.388985\ttraining's auc: 0.853182\n",
      "[774]\ttraining's binary_logloss: 0.388943\ttraining's auc: 0.853218\n",
      "[775]\ttraining's binary_logloss: 0.388898\ttraining's auc: 0.853277\n",
      "[776]\ttraining's binary_logloss: 0.388848\ttraining's auc: 0.853294\n",
      "[777]\ttraining's binary_logloss: 0.388808\ttraining's auc: 0.853358\n",
      "[778]\ttraining's binary_logloss: 0.388767\ttraining's auc: 0.853408\n",
      "[779]\ttraining's binary_logloss: 0.388725\ttraining's auc: 0.853459\n",
      "[780]\ttraining's binary_logloss: 0.388677\ttraining's auc: 0.8535\n",
      "[781]\ttraining's binary_logloss: 0.38863\ttraining's auc: 0.85352\n",
      "[782]\ttraining's binary_logloss: 0.388588\ttraining's auc: 0.85359\n",
      "[783]\ttraining's binary_logloss: 0.388544\ttraining's auc: 0.853634\n",
      "[784]\ttraining's binary_logloss: 0.3885\ttraining's auc: 0.853673\n",
      "[785]\ttraining's binary_logloss: 0.388461\ttraining's auc: 0.853706\n",
      "[786]\ttraining's binary_logloss: 0.388414\ttraining's auc: 0.853741\n",
      "[787]\ttraining's binary_logloss: 0.388371\ttraining's auc: 0.853786\n",
      "[788]\ttraining's binary_logloss: 0.388364\ttraining's auc: 0.853793\n",
      "[789]\ttraining's binary_logloss: 0.388353\ttraining's auc: 0.853799\n",
      "[790]\ttraining's binary_logloss: 0.388322\ttraining's auc: 0.853821\n",
      "[791]\ttraining's binary_logloss: 0.388272\ttraining's auc: 0.853868\n",
      "[792]\ttraining's binary_logloss: 0.38824\ttraining's auc: 0.853883\n",
      "[793]\ttraining's binary_logloss: 0.388216\ttraining's auc: 0.853894\n",
      "[794]\ttraining's binary_logloss: 0.388178\ttraining's auc: 0.853927\n",
      "[795]\ttraining's binary_logloss: 0.38814\ttraining's auc: 0.853943\n",
      "[796]\ttraining's binary_logloss: 0.388091\ttraining's auc: 0.854028\n",
      "[797]\ttraining's binary_logloss: 0.388044\ttraining's auc: 0.854082\n",
      "[798]\ttraining's binary_logloss: 0.388014\ttraining's auc: 0.854111\n",
      "[799]\ttraining's binary_logloss: 0.387973\ttraining's auc: 0.854125\n",
      "[800]\ttraining's binary_logloss: 0.387936\ttraining's auc: 0.854148\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[800]\ttraining's binary_logloss: 0.387936\ttraining's auc: 0.854148\n",
      "Best: 0.826510 using {'num_leaves': 40, 'n_estimators': 800, 'min_child_weight': 4, 'max_depth': 9, 'learning_rate': 0.05, 'colsample_bytree': 0.7000000000000002}\n"
     ]
    }
   ],
   "source": [
    "def execute_pipeline():\n",
    "    \n",
    "    df = read_data()\n",
    "    df = reduce_mem_usage(df, True)\n",
    "    df = transform_data(df)\n",
    "    df = feature_engineering(df, break_point)\n",
    "    run_lgb(df)\n",
    "\n",
    "execute_pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
