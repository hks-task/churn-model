{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Objective of the notebook:\n",
    "\n",
    "In this notebook, we create a machine learning model pipeline to predict probabilities and calculate model scores.\n",
    "- Importing libraries\n",
    "- Importing datasets\n",
    "- Change datatypes and reduce memory usage\n",
    "- Label encode categorical features (Features already encoded for this task.)\n",
    "- Fill null values\n",
    "- Calculate recency and number of days from the first order\n",
    "- Get time-related features like the year, month, week, day, day of the week, is a weekend\n",
    "- Add day differences between consecutive orders and \n",
    "- Calculate rolling features in 3 days, 1, 2, 4, 12, 24 weeks, and all time.\n",
    "- Convert raw data to a session format\n",
    "- Run the xgboost model\n",
    "- Calculate model scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-11T17:53:36.111668Z",
     "start_time": "2020-10-11T17:53:32.529047Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import xgboost\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\") \n",
    "\n",
    "kfold = KFold(n_splits=5, random_state=42)\n",
    "break_point = datetime(2017, 2, 28)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-11T17:53:36.135587Z",
     "start_time": "2020-10-11T17:53:36.120611Z"
    }
   },
   "outputs": [],
   "source": [
    "def read_data():\n",
    "    \n",
    "    print('Reading files...')    \n",
    "    order_df = pd.read_csv('../input/machine_learning_challenge_order_data.csv')\n",
    "    print('Order data has {} rows and {} columns'.format(order_df.shape[0], order_df.shape[1]))\n",
    "    label_df = pd.read_csv('../input/machine_learning_challenge_labeled_data.csv')\n",
    "    print('Label data has {} rows and {} columns'.format(label_df.shape[0], label_df.shape[1]))\n",
    "    df = order_df.merge(label_df, on='customer_id')\n",
    "    print('The final data has {} rows and {} columns'.format(df.shape[0], df.shape[1]))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Change data types and reduce memory usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-11T17:53:36.159019Z",
     "start_time": "2020-10-11T17:53:36.141046Z"
    }
   },
   "outputs": [],
   "source": [
    "def reduce_mem_usage(df, verbose=False):\n",
    "    \n",
    "    start_mem = df.memory_usage().sum() / 1024 ** 2\n",
    "    int_columns = df.select_dtypes(include=[\"int\"]).columns\n",
    "    float_columns = df.select_dtypes(include=[\"float\"]).columns\n",
    "\n",
    "    for col in int_columns:\n",
    "        df[col] = pd.to_numeric(df[col], downcast=\"integer\")\n",
    "\n",
    "    for col in float_columns:\n",
    "        df[col] = pd.to_numeric(df[col], downcast=\"float\")\n",
    "\n",
    "    end_mem = df.memory_usage().sum() / 1024 ** 2\n",
    "    if verbose:\n",
    "        print(\n",
    "            \"Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)\".format(\n",
    "                end_mem, 100 * (start_mem - end_mem) / start_mem\n",
    "            )\n",
    "        )\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Label encode categorical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-11T17:53:36.172933Z",
     "start_time": "2020-10-11T17:53:36.162465Z"
    }
   },
   "outputs": [],
   "source": [
    "def getWeeklyDates(df, break_point):\n",
    "\n",
    "    df['order_date'] = pd.to_datetime(df['order_date'])\n",
    "    three_day = df[df['order_date'] >= break_point - timedelta(days=3)]\n",
    "    one_week = df[df['order_date'] >= break_point - timedelta(days=7)]\n",
    "    two_week = df[df['order_date'] >= break_point - timedelta(days=14)]\n",
    "    four_week = df[df['order_date'] >= break_point - timedelta(days=28)]\n",
    "    twelve_week = df[df['order_date'] >= break_point - timedelta(days=84)]\n",
    "    twenty_four_week = df[df['order_date'] >= break_point - timedelta(days=168)]\n",
    "    all_week = df\n",
    "    return three_day, one_week, two_week, four_week, twelve_week, twenty_four_week,all_week"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-11T17:53:36.181795Z",
     "start_time": "2020-10-11T17:53:36.176727Z"
    }
   },
   "outputs": [],
   "source": [
    "def transform_data(df):\n",
    "\n",
    "    labelencoder = LabelEncoder()\n",
    "\n",
    "    for i in ['restaurant_id', 'city_id', 'payment_id', 'platform_id', 'transmission_id']:\n",
    "        df[i] = labelencoder.fit_transform(df[i])\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert raw data to a session format\n",
    "- Fill order rank with the forward-filling method. \n",
    "- Calculate recency and number of days from the first order.\n",
    "- Get time-related features like the year, month, week, day, day of the week, weekend.\n",
    "- Add day differences between consecutive orders. \n",
    "- Calculate rolling features in 3 days, 1, 2, 4, 12, 24 weeks, and all time.\n",
    "- Keep the last record of each customer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-11T17:53:36.214374Z",
     "start_time": "2020-10-11T17:53:36.184681Z"
    }
   },
   "outputs": [],
   "source": [
    "def feature_engineering(df, break_point):\n",
    "    \n",
    "    df['customer_order_rank'] = df['customer_order_rank'].fillna(method='ffill')\n",
    "\n",
    "    df['date'] = pd.to_datetime(df['order_date']) \n",
    "    df['recency'] = (break_point - df['date']) / np.timedelta64(1, 'D')\n",
    "    df['first_order_date'] = df.groupby(['customer_id'])['date'].transform('first')\n",
    "    df['age_of_user'] = (break_point - df['first_order_date']) / np.timedelta64(1, 'D')\n",
    "\n",
    "    df['year'] = df['date'].dt.year\n",
    "    df['month'] = df['date'].dt.month\n",
    "    df['week'] = df['date'].dt.week\n",
    "    df['day'] = df['date'].dt.day\n",
    "    df['dayofweek'] = df['date'].dt.dayofweek\n",
    "    df[\"is_weekend\"] = df[\"dayofweek\"].isin([5, 6]).astype(np.int8)\n",
    "    \n",
    "    df['demand'] = 1\n",
    "    \n",
    "    df['order_date_shift'] = df.groupby('customer_id')['date'].shift()\n",
    "    df['date_diff'] = (df['date'] - df['order_date_shift']) / np.timedelta64(1, 'D')\n",
    "\n",
    "    three_day, one_week, two_week, four_week, twelve_week, twenty_four_week,all_week = getWeeklyDates(df, break_point)\n",
    "    \n",
    "    col = ['demand', 'is_failed','voucher_amount','delivery_fee', 'amount_paid', 'date_diff']\n",
    "    three_day, one_week, two_week, four_week, twelve_week, twenty_four_week,all_week = getWeeklyDates(df, break_point)\n",
    "    three_day = three_day.groupby('customer_id')[col].mean().add_prefix('three_day_').reset_index()\n",
    "    one_week = one_week.groupby('customer_id')[col].mean().add_prefix('one_week_').reset_index()\n",
    "    two_week = two_week.groupby('customer_id')[col].mean().add_prefix('two_week_').reset_index()\n",
    "    four_week = four_week.groupby('customer_id')[col].mean().add_prefix('four_week_').reset_index()\n",
    "    twelve_week = twelve_week.groupby('customer_id')[col].mean().add_prefix('twelve_week_').reset_index()\n",
    "    twenty_four_week = twenty_four_week.groupby('customer_id')[col].mean().add_prefix('twenty_four_week_').reset_index()\n",
    "    all_week = all_week.groupby('customer_id')[col].mean().add_prefix('all_week_').reset_index()\n",
    "    \n",
    "    df = df.merge(three_day, how='left').merge(one_week, how='left').merge(two_week, how='left').merge(four_week,\n",
    "    'left').merge(twelve_week,'left').merge(twenty_four_week,'left').merge(all_week,'left').reset_index()\n",
    "\n",
    "    df = df.groupby('customer_id').last().reset_index()\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the xgb model and calculate scores\n",
    "\n",
    "We give more weights on class 1 using scale the pos weight parameter to label returned customers more correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-11T17:53:36.227879Z",
     "start_time": "2020-10-11T17:53:36.217115Z"
    }
   },
   "outputs": [],
   "source": [
    "def run_xgb(df):\n",
    "    \n",
    "    y = df['is_returning_customer']\n",
    "    X = df.drop(columns=['customer_id', 'order_date', 'date', 'is_returning_customer',\n",
    "                        'first_order_date', 'order_date_shift'])    \n",
    "    \n",
    "    clf = xgboost.XGBClassifier(objective= 'binary:logistic', n_jobs= -1, scale_pos_weight=2) \n",
    "    \n",
    "    y_pred = cross_val_predict(clf, X, y, cv=kfold)\n",
    "    \n",
    "    predicted = pd.DataFrame(y_pred, index=df.customer_id, columns=['y_pred']).reset_index()\n",
    "    predicted['y_actual'] = y\n",
    "    predicted = predicted.merge(df[['customer_id', 'customer_order_rank','recency']], on='customer_id')\n",
    "    #predicted.to_csv('pred.csv')\n",
    "\n",
    "    print('Accuracy Score:  ', round(metrics.accuracy_score(y, y_pred), 2))\n",
    "    print('Roc Auc Score:  ', round(roc_auc_score(y, y_pred), 2))\n",
    "    print('Classification Report: \\n', classification_report(y, y_pred, target_names=['0', '1']))\n",
    "    return predicted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execute all pipeline\n",
    "\n",
    "The accuracy and roc-auc scores are 0.83 and 0.73 relatively. Adding features increase model scores by 1%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-11T17:55:57.684777Z",
     "start_time": "2020-10-11T17:53:36.231339Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading files...\n",
      "Order data has 786600 rows and 13 columns\n",
      "Label data has 245455 rows and 2 columns\n",
      "The final data has 786600 rows and 14 columns\n",
      "Mem. usage decreased to 42.76 Mb (52.5% reduction)\n",
      "Accuracy Score:   0.83\n",
      "Roc Auc Score:   0.73\n",
      "Classification Report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.90      0.89    189948\n",
      "           1       0.63      0.57      0.59     55507\n",
      "\n",
      "   micro avg       0.83      0.83      0.83    245455\n",
      "   macro avg       0.75      0.73      0.74    245455\n",
      "weighted avg       0.82      0.83      0.82    245455\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def transform_train_and_eval():\n",
    "    \n",
    "    df = read_data()\n",
    "    df = reduce_mem_usage(df, True)\n",
    "    df = transform_data(df)\n",
    "    df = feature_engineering(df, break_point)\n",
    "    predicted = run_xgb(df)\n",
    "    return predicted\n",
    "\n",
    "predicted = transform_train_and_eval()"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
