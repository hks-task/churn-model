{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Objective of the notebook:\n",
    "\n",
    "In this notebook, we will try different parameter combinations for the final lightGBM classfier model to increase model performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-13T21:43:11.497949Z",
     "start_time": "2020-10-13T21:43:09.709674Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kaan.simsek/anaconda3/envs/forecasting/lib/python3.7/site-packages/lightgbm/__init__.py:46: UserWarning: Starting from version 2.2.1, the library file in distribution wheels for macOS is built by the Apple Clang (Xcode_8.3.3) compiler.\n",
      "This means that in case of installing LightGBM from PyPI via the ``pip install lightgbm`` command, you don't need to install the gcc compiler anymore.\n",
      "Instead of that, you need to install the OpenMP library, which is required for running LightGBM on the system with the Apple Clang compiler.\n",
      "You can install the OpenMP library by the following command: ``brew install libomp``.\n",
      "  \"You can install the OpenMP library by the following command: ``brew install libomp``.\", UserWarning)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import lightgbm \n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\") \n",
    "\n",
    "kfold = KFold(n_splits=5, random_state=42)\n",
    "break_point = datetime(2017, 2, 28)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-13T21:43:11.503789Z",
     "start_time": "2020-10-13T21:43:11.499567Z"
    }
   },
   "outputs": [],
   "source": [
    "def read_data():\n",
    "    \n",
    "    print('Reading files...')    \n",
    "    order_df = pd.read_csv('../input/machine_learning_challenge_order_data.csv')\n",
    "    print('Order data has {} rows and {} columns'.format(order_df.shape[0], order_df.shape[1]))\n",
    "    label_df = pd.read_csv('../input/machine_learning_challenge_labeled_data.csv')\n",
    "    print('Label data has {} rows and {} columns'.format(label_df.shape[0], label_df.shape[1]))\n",
    "    df = order_df.merge(label_df, on='customer_id')\n",
    "    print('The final data has {} rows and {} columns'.format(df.shape[0], df.shape[1]))\n",
    "    print(\"\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Change data types and reduce memory usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-13T21:43:11.510964Z",
     "start_time": "2020-10-13T21:43:11.505353Z"
    }
   },
   "outputs": [],
   "source": [
    "def reduce_mem_usage(df, verbose=False):\n",
    "    \n",
    "    start_mem = df.memory_usage().sum() / 1024 ** 2\n",
    "    int_columns = df.select_dtypes(include=[\"int\"]).columns\n",
    "    float_columns = df.select_dtypes(include=[\"float\"]).columns\n",
    "\n",
    "    for col in int_columns:\n",
    "        df[col] = pd.to_numeric(df[col], downcast=\"integer\")\n",
    "\n",
    "    for col in float_columns:\n",
    "        df[col] = pd.to_numeric(df[col], downcast=\"float\")\n",
    "\n",
    "    end_mem = df.memory_usage().sum() / 1024 ** 2\n",
    "    if verbose:\n",
    "        print(\n",
    "            \"Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)\".format(\n",
    "                end_mem, 100 * (start_mem - end_mem) / start_mem\n",
    "            )\n",
    "        )\n",
    "    print(\"\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Label encode categorical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-13T21:43:11.515256Z",
     "start_time": "2020-10-13T21:43:11.512548Z"
    }
   },
   "outputs": [],
   "source": [
    "def transform_data(df):\n",
    "\n",
    "    labelencoder = LabelEncoder()\n",
    "\n",
    "    for i in ['restaurant_id', 'city_id', 'payment_id', 'platform_id', 'transmission_id']:\n",
    "        df[i] = labelencoder.fit_transform(df[i])\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert raw data to a session format\n",
    "- Fill order rank with the forward-filling method. \n",
    "- Calculate recency and number of days from the first order.\n",
    "- Get time-related features like the year, month, week, day, day of the week, weekend.\n",
    "- Add day differences between consecutive orders. \n",
    "- Calculate rolling features in 3 days, 1, 2, 4, 12, 24 weeks, and all time.\n",
    "- Keep the last record of each customer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-13T21:43:11.522092Z",
     "start_time": "2020-10-13T21:43:11.517343Z"
    }
   },
   "outputs": [],
   "source": [
    "def getWeeklyDates(df, break_point):\n",
    "\n",
    "    df['order_date'] = pd.to_datetime(df['order_date'])\n",
    "    three_day = df[df['order_date'] >= break_point - timedelta(days=3)]\n",
    "    one_week = df[df['order_date'] >= break_point - timedelta(days=7)]\n",
    "    two_week = df[df['order_date'] >= break_point - timedelta(days=14)]\n",
    "    four_week = df[df['order_date'] >= break_point - timedelta(days=28)]\n",
    "    twelve_week = df[df['order_date'] >= break_point - timedelta(days=84)]\n",
    "    twenty_four_week = df[df['order_date'] >= break_point - timedelta(days=168)]\n",
    "    all_week = df\n",
    "    return three_day, one_week, two_week, four_week, twelve_week, twenty_four_week,all_week"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-13T21:43:11.539317Z",
     "start_time": "2020-10-13T21:43:11.523874Z"
    }
   },
   "outputs": [],
   "source": [
    "def feature_engineering(df, break_point):\n",
    "    \n",
    "    df['customer_order_rank'] = df['customer_order_rank'].fillna(method='ffill')\n",
    "\n",
    "    df['date'] = pd.to_datetime(df['order_date']) \n",
    "    df['recency'] = (break_point - df['date']) / np.timedelta64(1, 'D')\n",
    "    df['first_order_date'] = df.groupby(['customer_id'])['date'].transform('first')\n",
    "    df['age_of_user'] = (break_point - df['first_order_date']) / np.timedelta64(1, 'D')\n",
    "\n",
    "    df['year'] = df['date'].dt.year\n",
    "    df['month'] = df['date'].dt.month\n",
    "    df['week'] = df['date'].dt.week\n",
    "    df['day'] = df['date'].dt.day\n",
    "    df['dayofweek'] = df['date'].dt.dayofweek\n",
    "    df[\"is_weekend\"] = df[\"dayofweek\"].isin([5, 6]).astype(np.int8)\n",
    "    \n",
    "    df['demand'] = 1\n",
    "    \n",
    "    df['order_date_shift'] = df.groupby('customer_id')['date'].shift()\n",
    "    df['date_diff'] = (df['date'] - df['order_date_shift']) / np.timedelta64(1, 'D')\n",
    "\n",
    "    three_day, one_week, two_week, four_week, twelve_week, twenty_four_week,all_week = getWeeklyDates(df, break_point)\n",
    "    \n",
    "    col = ['demand', 'is_failed','voucher_amount','delivery_fee', 'amount_paid', 'date_diff']\n",
    "    three_day, one_week, two_week, four_week, twelve_week, twenty_four_week,all_week = getWeeklyDates(df, break_point)\n",
    "    three_day = three_day.groupby('customer_id')[col].mean().add_prefix('three_day_').reset_index()\n",
    "    one_week = one_week.groupby('customer_id')[col].mean().add_prefix('one_week_').reset_index()\n",
    "    two_week = two_week.groupby('customer_id')[col].mean().add_prefix('two_week_').reset_index()\n",
    "    four_week = four_week.groupby('customer_id')[col].mean().add_prefix('four_week_').reset_index()\n",
    "    twelve_week = twelve_week.groupby('customer_id')[col].mean().add_prefix('twelve_week_').reset_index()\n",
    "    twenty_four_week = twenty_four_week.groupby('customer_id')[col].mean().add_prefix('twenty_four_week_').reset_index()\n",
    "    all_week = all_week.groupby('customer_id')[col].mean().add_prefix('all_week_').reset_index()\n",
    "    \n",
    "    df = df.groupby('customer_id').last().reset_index()\n",
    "    df = df.merge(three_day, how='left').merge(one_week, how='left').merge(two_week, how='left').merge(four_week,\n",
    "    'left').merge(twelve_week,'left').merge(twenty_four_week,'left').merge(all_week,'left').reset_index()\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Search best parameters for the final lightGBM model and features\n",
    "\n",
    "We give more weights on class 1 using scale the pos weight parameter to label returned customers more correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-13T21:43:11.549199Z",
     "start_time": "2020-10-13T21:43:11.541239Z"
    }
   },
   "outputs": [],
   "source": [
    "def run_lgb(df):\n",
    "    \n",
    "    y = df['is_returning_customer']\n",
    "    X = df.drop(columns=['customer_id', 'order_date', 'date', 'is_returning_customer',\n",
    "                        'first_order_date', 'index', 'order_date_shift'])    \n",
    "    \n",
    "    clf = lightgbm.LGBMClassifier(n_jobs= -1, scale_pos_weight=2)\n",
    "    \n",
    "    param_dist = {\n",
    "                    'max_depth': np.arange(3,15,3),\n",
    "                    'min_child_weight': np.arange(1,8,1),\n",
    "                    'colsample_bytree': np.arange(0.3,0.9,0.1),\n",
    "                    'n_estimators': np.arange(100,1000,100),\n",
    "                    'learning_rate': np.arange(0.05,0.3,0.05),\n",
    "                    'num_leaves':  np.arange(10,100,10)\n",
    "             }\n",
    "    \n",
    "    fit_params={\"early_stopping_rounds\":10,\n",
    "               \"eval_metric\" : \"auc\", \n",
    "               \"eval_set\" : [[X, y]]}\n",
    "\n",
    "    grid_search = RandomizedSearchCV(clf, \n",
    "                         param_distributions = param_dist,\n",
    "                         cv = kfold,  \n",
    "                         n_iter = 50,\n",
    "                         verbose = 0, \n",
    "                         n_jobs = -1,\n",
    "                         fit_params=fit_params)\n",
    "    \n",
    "    grid_result = grid_search.fit(X,y)\n",
    "\n",
    "    print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "    means = grid_result.cv_results_[ 'mean_test_score' ]\n",
    "    stds = grid_result.cv_results_[ 'std_test_score' ]\n",
    "    params = grid_result.cv_results_[ 'params' ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-13T22:20:09.171746Z",
     "start_time": "2020-10-13T21:43:11.551109Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading files...\n",
      "Order data has 786600 rows and 13 columns\n",
      "Label data has 245455 rows and 2 columns\n",
      "The final data has 786600 rows and 14 columns\n",
      "\n",
      "Mem. usage decreased to 42.76 Mb (52.5% reduction)\n",
      "\n",
      "[1]\ttraining's binary_logloss: 0.519133\ttraining's auc: 0.814505\n",
      "Training until validation scores don't improve for 10 rounds.\n",
      "[2]\ttraining's binary_logloss: 0.506316\ttraining's auc: 0.817914\n",
      "[3]\ttraining's binary_logloss: 0.495534\ttraining's auc: 0.818823\n",
      "[4]\ttraining's binary_logloss: 0.486532\ttraining's auc: 0.819195\n",
      "[5]\ttraining's binary_logloss: 0.478628\ttraining's auc: 0.819598\n",
      "[6]\ttraining's binary_logloss: 0.471959\ttraining's auc: 0.819793\n",
      "[7]\ttraining's binary_logloss: 0.465952\ttraining's auc: 0.820079\n",
      "[8]\ttraining's binary_logloss: 0.46077\ttraining's auc: 0.820255\n",
      "[9]\ttraining's binary_logloss: 0.45639\ttraining's auc: 0.820438\n",
      "[10]\ttraining's binary_logloss: 0.452182\ttraining's auc: 0.820737\n",
      "[11]\ttraining's binary_logloss: 0.448574\ttraining's auc: 0.82085\n",
      "[12]\ttraining's binary_logloss: 0.445267\ttraining's auc: 0.821005\n",
      "[13]\ttraining's binary_logloss: 0.442339\ttraining's auc: 0.821183\n",
      "[14]\ttraining's binary_logloss: 0.439731\ttraining's auc: 0.821404\n",
      "[15]\ttraining's binary_logloss: 0.43735\ttraining's auc: 0.821501\n",
      "[16]\ttraining's binary_logloss: 0.435355\ttraining's auc: 0.821591\n",
      "[17]\ttraining's binary_logloss: 0.433513\ttraining's auc: 0.821704\n",
      "[18]\ttraining's binary_logloss: 0.431766\ttraining's auc: 0.82182\n",
      "[19]\ttraining's binary_logloss: 0.43026\ttraining's auc: 0.821964\n",
      "[20]\ttraining's binary_logloss: 0.428912\ttraining's auc: 0.822039\n",
      "[21]\ttraining's binary_logloss: 0.427642\ttraining's auc: 0.822117\n",
      "[22]\ttraining's binary_logloss: 0.42653\ttraining's auc: 0.822197\n",
      "[23]\ttraining's binary_logloss: 0.425504\ttraining's auc: 0.82234\n",
      "[24]\ttraining's binary_logloss: 0.424577\ttraining's auc: 0.822437\n",
      "[25]\ttraining's binary_logloss: 0.423765\ttraining's auc: 0.822608\n",
      "[26]\ttraining's binary_logloss: 0.42303\ttraining's auc: 0.822735\n",
      "[27]\ttraining's binary_logloss: 0.422381\ttraining's auc: 0.822862\n",
      "[28]\ttraining's binary_logloss: 0.421773\ttraining's auc: 0.823008\n",
      "[29]\ttraining's binary_logloss: 0.421308\ttraining's auc: 0.823136\n",
      "[30]\ttraining's binary_logloss: 0.420805\ttraining's auc: 0.823264\n",
      "[31]\ttraining's binary_logloss: 0.42036\ttraining's auc: 0.823348\n",
      "[32]\ttraining's binary_logloss: 0.419964\ttraining's auc: 0.823442\n",
      "[33]\ttraining's binary_logloss: 0.419599\ttraining's auc: 0.823545\n",
      "[34]\ttraining's binary_logloss: 0.419296\ttraining's auc: 0.823599\n",
      "[35]\ttraining's binary_logloss: 0.419012\ttraining's auc: 0.823703\n",
      "[36]\ttraining's binary_logloss: 0.418766\ttraining's auc: 0.823823\n",
      "[37]\ttraining's binary_logloss: 0.418533\ttraining's auc: 0.823917\n",
      "[38]\ttraining's binary_logloss: 0.41831\ttraining's auc: 0.824019\n",
      "[39]\ttraining's binary_logloss: 0.418109\ttraining's auc: 0.824175\n",
      "[40]\ttraining's binary_logloss: 0.417927\ttraining's auc: 0.824289\n",
      "[41]\ttraining's binary_logloss: 0.417747\ttraining's auc: 0.824415\n",
      "[42]\ttraining's binary_logloss: 0.417603\ttraining's auc: 0.824489\n",
      "[43]\ttraining's binary_logloss: 0.417499\ttraining's auc: 0.82457\n",
      "[44]\ttraining's binary_logloss: 0.417372\ttraining's auc: 0.824701\n",
      "[45]\ttraining's binary_logloss: 0.417261\ttraining's auc: 0.824791\n",
      "[46]\ttraining's binary_logloss: 0.41716\ttraining's auc: 0.82495\n",
      "[47]\ttraining's binary_logloss: 0.417069\ttraining's auc: 0.825072\n",
      "[48]\ttraining's binary_logloss: 0.416997\ttraining's auc: 0.825153\n",
      "[49]\ttraining's binary_logloss: 0.416915\ttraining's auc: 0.825238\n",
      "[50]\ttraining's binary_logloss: 0.416845\ttraining's auc: 0.825335\n",
      "[51]\ttraining's binary_logloss: 0.416759\ttraining's auc: 0.825468\n",
      "[52]\ttraining's binary_logloss: 0.416733\ttraining's auc: 0.825574\n",
      "[53]\ttraining's binary_logloss: 0.416649\ttraining's auc: 0.825711\n",
      "[54]\ttraining's binary_logloss: 0.416574\ttraining's auc: 0.825827\n",
      "[55]\ttraining's binary_logloss: 0.416518\ttraining's auc: 0.825942\n",
      "[56]\ttraining's binary_logloss: 0.416462\ttraining's auc: 0.826012\n",
      "[57]\ttraining's binary_logloss: 0.416416\ttraining's auc: 0.82616\n",
      "[58]\ttraining's binary_logloss: 0.416338\ttraining's auc: 0.826286\n",
      "[59]\ttraining's binary_logloss: 0.416261\ttraining's auc: 0.826403\n",
      "[60]\ttraining's binary_logloss: 0.416197\ttraining's auc: 0.826494\n",
      "[61]\ttraining's binary_logloss: 0.416159\ttraining's auc: 0.826605\n",
      "[62]\ttraining's binary_logloss: 0.416081\ttraining's auc: 0.826733\n",
      "[63]\ttraining's binary_logloss: 0.41601\ttraining's auc: 0.826856\n",
      "[64]\ttraining's binary_logloss: 0.415932\ttraining's auc: 0.826974\n",
      "[65]\ttraining's binary_logloss: 0.41587\ttraining's auc: 0.827096\n",
      "[66]\ttraining's binary_logloss: 0.415798\ttraining's auc: 0.827204\n",
      "[67]\ttraining's binary_logloss: 0.415722\ttraining's auc: 0.827313\n",
      "[68]\ttraining's binary_logloss: 0.415644\ttraining's auc: 0.827448\n",
      "[69]\ttraining's binary_logloss: 0.415566\ttraining's auc: 0.827574\n",
      "[70]\ttraining's binary_logloss: 0.415504\ttraining's auc: 0.827685\n",
      "[71]\ttraining's binary_logloss: 0.415447\ttraining's auc: 0.827773\n",
      "[72]\ttraining's binary_logloss: 0.41537\ttraining's auc: 0.82788\n",
      "[73]\ttraining's binary_logloss: 0.415293\ttraining's auc: 0.828019\n",
      "[74]\ttraining's binary_logloss: 0.415232\ttraining's auc: 0.82812\n",
      "[75]\ttraining's binary_logloss: 0.41518\ttraining's auc: 0.8282\n",
      "[76]\ttraining's binary_logloss: 0.415099\ttraining's auc: 0.828298\n",
      "[77]\ttraining's binary_logloss: 0.415021\ttraining's auc: 0.828373\n",
      "[78]\ttraining's binary_logloss: 0.414942\ttraining's auc: 0.828458\n",
      "[79]\ttraining's binary_logloss: 0.414865\ttraining's auc: 0.828529\n",
      "[80]\ttraining's binary_logloss: 0.414795\ttraining's auc: 0.828604\n",
      "[81]\ttraining's binary_logloss: 0.414721\ttraining's auc: 0.828686\n",
      "[82]\ttraining's binary_logloss: 0.414645\ttraining's auc: 0.828772\n",
      "[83]\ttraining's binary_logloss: 0.414582\ttraining's auc: 0.828834\n",
      "[84]\ttraining's binary_logloss: 0.414505\ttraining's auc: 0.8289\n",
      "[85]\ttraining's binary_logloss: 0.414432\ttraining's auc: 0.828987\n",
      "[86]\ttraining's binary_logloss: 0.414364\ttraining's auc: 0.829073\n",
      "[87]\ttraining's binary_logloss: 0.414275\ttraining's auc: 0.82918\n",
      "[88]\ttraining's binary_logloss: 0.414191\ttraining's auc: 0.829296\n",
      "[89]\ttraining's binary_logloss: 0.414093\ttraining's auc: 0.829411\n",
      "[90]\ttraining's binary_logloss: 0.414016\ttraining's auc: 0.829503\n",
      "[91]\ttraining's binary_logloss: 0.413929\ttraining's auc: 0.829585\n",
      "[92]\ttraining's binary_logloss: 0.413831\ttraining's auc: 0.829701\n",
      "[93]\ttraining's binary_logloss: 0.41375\ttraining's auc: 0.82978\n",
      "[94]\ttraining's binary_logloss: 0.413663\ttraining's auc: 0.829842\n",
      "[95]\ttraining's binary_logloss: 0.413567\ttraining's auc: 0.829949\n",
      "[96]\ttraining's binary_logloss: 0.413468\ttraining's auc: 0.830076\n",
      "[97]\ttraining's binary_logloss: 0.413395\ttraining's auc: 0.830128\n",
      "[98]\ttraining's binary_logloss: 0.413317\ttraining's auc: 0.830193\n",
      "[99]\ttraining's binary_logloss: 0.41322\ttraining's auc: 0.830313\n",
      "[100]\ttraining's binary_logloss: 0.41314\ttraining's auc: 0.83038\n",
      "[101]\ttraining's binary_logloss: 0.413055\ttraining's auc: 0.830443\n",
      "[102]\ttraining's binary_logloss: 0.412941\ttraining's auc: 0.830577\n",
      "[103]\ttraining's binary_logloss: 0.412857\ttraining's auc: 0.830661\n",
      "[104]\ttraining's binary_logloss: 0.412778\ttraining's auc: 0.830723\n",
      "[105]\ttraining's binary_logloss: 0.412692\ttraining's auc: 0.83085\n",
      "[106]\ttraining's binary_logloss: 0.412596\ttraining's auc: 0.830956\n",
      "[107]\ttraining's binary_logloss: 0.412511\ttraining's auc: 0.831051\n",
      "[108]\ttraining's binary_logloss: 0.412425\ttraining's auc: 0.831146\n",
      "[109]\ttraining's binary_logloss: 0.412325\ttraining's auc: 0.831222\n",
      "[110]\ttraining's binary_logloss: 0.412217\ttraining's auc: 0.831351\n",
      "[111]\ttraining's binary_logloss: 0.412125\ttraining's auc: 0.831445\n",
      "[112]\ttraining's binary_logloss: 0.412071\ttraining's auc: 0.831502\n",
      "[113]\ttraining's binary_logloss: 0.411974\ttraining's auc: 0.83158\n",
      "[114]\ttraining's binary_logloss: 0.411889\ttraining's auc: 0.831653\n",
      "[115]\ttraining's binary_logloss: 0.411779\ttraining's auc: 0.831753\n",
      "[116]\ttraining's binary_logloss: 0.411681\ttraining's auc: 0.831838\n",
      "[117]\ttraining's binary_logloss: 0.411605\ttraining's auc: 0.831895\n",
      "[118]\ttraining's binary_logloss: 0.411556\ttraining's auc: 0.831946\n",
      "[119]\ttraining's binary_logloss: 0.411443\ttraining's auc: 0.832059\n",
      "[120]\ttraining's binary_logloss: 0.411335\ttraining's auc: 0.83219\n",
      "[121]\ttraining's binary_logloss: 0.41125\ttraining's auc: 0.832288\n",
      "[122]\ttraining's binary_logloss: 0.411159\ttraining's auc: 0.832406\n",
      "[123]\ttraining's binary_logloss: 0.411048\ttraining's auc: 0.832509\n",
      "[124]\ttraining's binary_logloss: 0.410966\ttraining's auc: 0.832572\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[125]\ttraining's binary_logloss: 0.410869\ttraining's auc: 0.832641\n",
      "[126]\ttraining's binary_logloss: 0.410791\ttraining's auc: 0.8327\n",
      "[127]\ttraining's binary_logloss: 0.410698\ttraining's auc: 0.832842\n",
      "[128]\ttraining's binary_logloss: 0.410593\ttraining's auc: 0.832928\n",
      "[129]\ttraining's binary_logloss: 0.410514\ttraining's auc: 0.833046\n",
      "[130]\ttraining's binary_logloss: 0.410431\ttraining's auc: 0.833122\n",
      "[131]\ttraining's binary_logloss: 0.410355\ttraining's auc: 0.833241\n",
      "[132]\ttraining's binary_logloss: 0.410243\ttraining's auc: 0.833339\n",
      "[133]\ttraining's binary_logloss: 0.410149\ttraining's auc: 0.833466\n",
      "[134]\ttraining's binary_logloss: 0.410059\ttraining's auc: 0.833539\n",
      "[135]\ttraining's binary_logloss: 0.410035\ttraining's auc: 0.833576\n",
      "[136]\ttraining's binary_logloss: 0.409974\ttraining's auc: 0.833612\n",
      "[137]\ttraining's binary_logloss: 0.409904\ttraining's auc: 0.833671\n",
      "[138]\ttraining's binary_logloss: 0.409815\ttraining's auc: 0.833818\n",
      "[139]\ttraining's binary_logloss: 0.409704\ttraining's auc: 0.833944\n",
      "[140]\ttraining's binary_logloss: 0.40964\ttraining's auc: 0.833969\n",
      "[141]\ttraining's binary_logloss: 0.409585\ttraining's auc: 0.834005\n",
      "[142]\ttraining's binary_logloss: 0.409491\ttraining's auc: 0.834117\n",
      "[143]\ttraining's binary_logloss: 0.409441\ttraining's auc: 0.834156\n",
      "[144]\ttraining's binary_logloss: 0.409361\ttraining's auc: 0.834279\n",
      "[145]\ttraining's binary_logloss: 0.409274\ttraining's auc: 0.83434\n",
      "[146]\ttraining's binary_logloss: 0.409232\ttraining's auc: 0.834382\n",
      "[147]\ttraining's binary_logloss: 0.409132\ttraining's auc: 0.83449\n",
      "[148]\ttraining's binary_logloss: 0.409062\ttraining's auc: 0.834519\n",
      "[149]\ttraining's binary_logloss: 0.408977\ttraining's auc: 0.834619\n",
      "[150]\ttraining's binary_logloss: 0.408942\ttraining's auc: 0.834656\n",
      "[151]\ttraining's binary_logloss: 0.408876\ttraining's auc: 0.834726\n",
      "[152]\ttraining's binary_logloss: 0.408789\ttraining's auc: 0.834807\n",
      "[153]\ttraining's binary_logloss: 0.408731\ttraining's auc: 0.834861\n",
      "[154]\ttraining's binary_logloss: 0.408628\ttraining's auc: 0.834965\n",
      "[155]\ttraining's binary_logloss: 0.408528\ttraining's auc: 0.835077\n",
      "[156]\ttraining's binary_logloss: 0.408441\ttraining's auc: 0.835154\n",
      "[157]\ttraining's binary_logloss: 0.408347\ttraining's auc: 0.835276\n",
      "[158]\ttraining's binary_logloss: 0.408246\ttraining's auc: 0.835404\n",
      "[159]\ttraining's binary_logloss: 0.408166\ttraining's auc: 0.835508\n",
      "[160]\ttraining's binary_logloss: 0.408067\ttraining's auc: 0.835621\n",
      "[161]\ttraining's binary_logloss: 0.407965\ttraining's auc: 0.835717\n",
      "[162]\ttraining's binary_logloss: 0.407912\ttraining's auc: 0.835775\n",
      "[163]\ttraining's binary_logloss: 0.407813\ttraining's auc: 0.835866\n",
      "[164]\ttraining's binary_logloss: 0.407742\ttraining's auc: 0.835936\n",
      "[165]\ttraining's binary_logloss: 0.407651\ttraining's auc: 0.836059\n",
      "[166]\ttraining's binary_logloss: 0.407551\ttraining's auc: 0.836203\n",
      "[167]\ttraining's binary_logloss: 0.407501\ttraining's auc: 0.836241\n",
      "[168]\ttraining's binary_logloss: 0.407472\ttraining's auc: 0.836274\n",
      "[169]\ttraining's binary_logloss: 0.407424\ttraining's auc: 0.83634\n",
      "[170]\ttraining's binary_logloss: 0.407344\ttraining's auc: 0.836444\n",
      "[171]\ttraining's binary_logloss: 0.407318\ttraining's auc: 0.836462\n",
      "[172]\ttraining's binary_logloss: 0.407273\ttraining's auc: 0.83648\n",
      "[173]\ttraining's binary_logloss: 0.407183\ttraining's auc: 0.836565\n",
      "[174]\ttraining's binary_logloss: 0.40711\ttraining's auc: 0.836626\n",
      "[175]\ttraining's binary_logloss: 0.407086\ttraining's auc: 0.836655\n",
      "[176]\ttraining's binary_logloss: 0.40699\ttraining's auc: 0.836739\n",
      "[177]\ttraining's binary_logloss: 0.406889\ttraining's auc: 0.836805\n",
      "[178]\ttraining's binary_logloss: 0.406782\ttraining's auc: 0.836915\n",
      "[179]\ttraining's binary_logloss: 0.406709\ttraining's auc: 0.837033\n",
      "[180]\ttraining's binary_logloss: 0.406673\ttraining's auc: 0.837062\n",
      "[181]\ttraining's binary_logloss: 0.406568\ttraining's auc: 0.837132\n",
      "[182]\ttraining's binary_logloss: 0.406484\ttraining's auc: 0.837224\n",
      "[183]\ttraining's binary_logloss: 0.406388\ttraining's auc: 0.837317\n",
      "[184]\ttraining's binary_logloss: 0.406334\ttraining's auc: 0.837342\n",
      "[185]\ttraining's binary_logloss: 0.406283\ttraining's auc: 0.837388\n",
      "[186]\ttraining's binary_logloss: 0.406198\ttraining's auc: 0.837455\n",
      "[187]\ttraining's binary_logloss: 0.4061\ttraining's auc: 0.837525\n",
      "[188]\ttraining's binary_logloss: 0.406045\ttraining's auc: 0.837576\n",
      "[189]\ttraining's binary_logloss: 0.405952\ttraining's auc: 0.837681\n",
      "[190]\ttraining's binary_logloss: 0.405864\ttraining's auc: 0.837785\n",
      "[191]\ttraining's binary_logloss: 0.405785\ttraining's auc: 0.837833\n",
      "[192]\ttraining's binary_logloss: 0.405718\ttraining's auc: 0.837879\n",
      "[193]\ttraining's binary_logloss: 0.405683\ttraining's auc: 0.837916\n",
      "[194]\ttraining's binary_logloss: 0.405626\ttraining's auc: 0.837979\n",
      "[195]\ttraining's binary_logloss: 0.40554\ttraining's auc: 0.838077\n",
      "[196]\ttraining's binary_logloss: 0.40546\ttraining's auc: 0.83813\n",
      "[197]\ttraining's binary_logloss: 0.405369\ttraining's auc: 0.838203\n",
      "[198]\ttraining's binary_logloss: 0.405312\ttraining's auc: 0.838257\n",
      "[199]\ttraining's binary_logloss: 0.405242\ttraining's auc: 0.8383\n",
      "[200]\ttraining's binary_logloss: 0.405214\ttraining's auc: 0.838331\n",
      "[201]\ttraining's binary_logloss: 0.405124\ttraining's auc: 0.838441\n",
      "[202]\ttraining's binary_logloss: 0.405015\ttraining's auc: 0.83857\n",
      "[203]\ttraining's binary_logloss: 0.405\ttraining's auc: 0.83858\n",
      "[204]\ttraining's binary_logloss: 0.404955\ttraining's auc: 0.838633\n",
      "[205]\ttraining's binary_logloss: 0.404882\ttraining's auc: 0.838715\n",
      "[206]\ttraining's binary_logloss: 0.404797\ttraining's auc: 0.838804\n",
      "[207]\ttraining's binary_logloss: 0.404716\ttraining's auc: 0.838845\n",
      "[208]\ttraining's binary_logloss: 0.404642\ttraining's auc: 0.838899\n",
      "[209]\ttraining's binary_logloss: 0.404607\ttraining's auc: 0.838915\n",
      "[210]\ttraining's binary_logloss: 0.404516\ttraining's auc: 0.83901\n",
      "[211]\ttraining's binary_logloss: 0.404443\ttraining's auc: 0.839054\n",
      "[212]\ttraining's binary_logloss: 0.404403\ttraining's auc: 0.839089\n",
      "[213]\ttraining's binary_logloss: 0.404313\ttraining's auc: 0.839181\n",
      "[214]\ttraining's binary_logloss: 0.404243\ttraining's auc: 0.839228\n",
      "[215]\ttraining's binary_logloss: 0.404227\ttraining's auc: 0.839237\n",
      "[216]\ttraining's binary_logloss: 0.404198\ttraining's auc: 0.839257\n",
      "[217]\ttraining's binary_logloss: 0.404085\ttraining's auc: 0.839363\n",
      "[218]\ttraining's binary_logloss: 0.404002\ttraining's auc: 0.839406\n",
      "[219]\ttraining's binary_logloss: 0.403933\ttraining's auc: 0.839494\n",
      "[220]\ttraining's binary_logloss: 0.403867\ttraining's auc: 0.839557\n",
      "[221]\ttraining's binary_logloss: 0.403759\ttraining's auc: 0.839677\n",
      "[222]\ttraining's binary_logloss: 0.40366\ttraining's auc: 0.839783\n",
      "[223]\ttraining's binary_logloss: 0.403554\ttraining's auc: 0.839902\n",
      "[224]\ttraining's binary_logloss: 0.40348\ttraining's auc: 0.840019\n",
      "[225]\ttraining's binary_logloss: 0.403444\ttraining's auc: 0.840063\n",
      "[226]\ttraining's binary_logloss: 0.403356\ttraining's auc: 0.840141\n",
      "[227]\ttraining's binary_logloss: 0.403334\ttraining's auc: 0.840156\n",
      "[228]\ttraining's binary_logloss: 0.403235\ttraining's auc: 0.840266\n",
      "[229]\ttraining's binary_logloss: 0.403194\ttraining's auc: 0.840292\n",
      "[230]\ttraining's binary_logloss: 0.403146\ttraining's auc: 0.840312\n",
      "[231]\ttraining's binary_logloss: 0.403126\ttraining's auc: 0.840323\n",
      "[232]\ttraining's binary_logloss: 0.403022\ttraining's auc: 0.840431\n",
      "[233]\ttraining's binary_logloss: 0.402944\ttraining's auc: 0.840485\n",
      "[234]\ttraining's binary_logloss: 0.402877\ttraining's auc: 0.840528\n",
      "[235]\ttraining's binary_logloss: 0.402797\ttraining's auc: 0.840635\n",
      "[236]\ttraining's binary_logloss: 0.40277\ttraining's auc: 0.840661\n",
      "[237]\ttraining's binary_logloss: 0.402722\ttraining's auc: 0.840693\n",
      "[238]\ttraining's binary_logloss: 0.40266\ttraining's auc: 0.840753\n",
      "[239]\ttraining's binary_logloss: 0.402616\ttraining's auc: 0.840801\n",
      "[240]\ttraining's binary_logloss: 0.402549\ttraining's auc: 0.84087\n",
      "[241]\ttraining's binary_logloss: 0.402507\ttraining's auc: 0.840896\n",
      "[242]\ttraining's binary_logloss: 0.402419\ttraining's auc: 0.84099\n",
      "[243]\ttraining's binary_logloss: 0.40241\ttraining's auc: 0.841\n",
      "[244]\ttraining's binary_logloss: 0.402322\ttraining's auc: 0.841069\n",
      "[245]\ttraining's binary_logloss: 0.402207\ttraining's auc: 0.841183\n",
      "[246]\ttraining's binary_logloss: 0.402109\ttraining's auc: 0.841323\n",
      "[247]\ttraining's binary_logloss: 0.402041\ttraining's auc: 0.841352\n",
      "[248]\ttraining's binary_logloss: 0.401932\ttraining's auc: 0.841456\n",
      "[249]\ttraining's binary_logloss: 0.401894\ttraining's auc: 0.841498\n",
      "[250]\ttraining's binary_logloss: 0.401809\ttraining's auc: 0.841556\n",
      "[251]\ttraining's binary_logloss: 0.401727\ttraining's auc: 0.841654\n",
      "[252]\ttraining's binary_logloss: 0.401679\ttraining's auc: 0.84171\n",
      "[253]\ttraining's binary_logloss: 0.401607\ttraining's auc: 0.841742\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[254]\ttraining's binary_logloss: 0.401584\ttraining's auc: 0.841756\n",
      "[255]\ttraining's binary_logloss: 0.401524\ttraining's auc: 0.841817\n",
      "[256]\ttraining's binary_logloss: 0.401477\ttraining's auc: 0.841854\n",
      "[257]\ttraining's binary_logloss: 0.401391\ttraining's auc: 0.841937\n",
      "[258]\ttraining's binary_logloss: 0.401334\ttraining's auc: 0.841958\n",
      "[259]\ttraining's binary_logloss: 0.40125\ttraining's auc: 0.842053\n",
      "[260]\ttraining's binary_logloss: 0.401178\ttraining's auc: 0.842106\n",
      "[261]\ttraining's binary_logloss: 0.40112\ttraining's auc: 0.842128\n",
      "[262]\ttraining's binary_logloss: 0.401035\ttraining's auc: 0.842233\n",
      "[263]\ttraining's binary_logloss: 0.40094\ttraining's auc: 0.842307\n",
      "[264]\ttraining's binary_logloss: 0.400838\ttraining's auc: 0.842425\n",
      "[265]\ttraining's binary_logloss: 0.400779\ttraining's auc: 0.842455\n",
      "[266]\ttraining's binary_logloss: 0.400701\ttraining's auc: 0.84251\n",
      "[267]\ttraining's binary_logloss: 0.400614\ttraining's auc: 0.842607\n",
      "[268]\ttraining's binary_logloss: 0.400558\ttraining's auc: 0.842631\n",
      "[269]\ttraining's binary_logloss: 0.400536\ttraining's auc: 0.842653\n",
      "[270]\ttraining's binary_logloss: 0.40045\ttraining's auc: 0.842759\n",
      "[271]\ttraining's binary_logloss: 0.400357\ttraining's auc: 0.842793\n",
      "[272]\ttraining's binary_logloss: 0.400267\ttraining's auc: 0.842897\n",
      "[273]\ttraining's binary_logloss: 0.400178\ttraining's auc: 0.842948\n",
      "[274]\ttraining's binary_logloss: 0.400086\ttraining's auc: 0.84305\n",
      "[275]\ttraining's binary_logloss: 0.400005\ttraining's auc: 0.843137\n",
      "[276]\ttraining's binary_logloss: 0.399903\ttraining's auc: 0.843215\n",
      "[277]\ttraining's binary_logloss: 0.399822\ttraining's auc: 0.843294\n",
      "[278]\ttraining's binary_logloss: 0.399754\ttraining's auc: 0.843364\n",
      "[279]\ttraining's binary_logloss: 0.399665\ttraining's auc: 0.843433\n",
      "[280]\ttraining's binary_logloss: 0.39957\ttraining's auc: 0.843463\n",
      "[281]\ttraining's binary_logloss: 0.399472\ttraining's auc: 0.843578\n",
      "[282]\ttraining's binary_logloss: 0.399418\ttraining's auc: 0.843613\n",
      "[283]\ttraining's binary_logloss: 0.399396\ttraining's auc: 0.84364\n",
      "[284]\ttraining's binary_logloss: 0.399303\ttraining's auc: 0.843742\n",
      "[285]\ttraining's binary_logloss: 0.399244\ttraining's auc: 0.843766\n",
      "[286]\ttraining's binary_logloss: 0.399171\ttraining's auc: 0.843862\n",
      "[287]\ttraining's binary_logloss: 0.3991\ttraining's auc: 0.843893\n",
      "[288]\ttraining's binary_logloss: 0.399086\ttraining's auc: 0.843904\n",
      "[289]\ttraining's binary_logloss: 0.398988\ttraining's auc: 0.84399\n",
      "[290]\ttraining's binary_logloss: 0.398888\ttraining's auc: 0.844102\n",
      "[291]\ttraining's binary_logloss: 0.3988\ttraining's auc: 0.844206\n",
      "[292]\ttraining's binary_logloss: 0.398723\ttraining's auc: 0.84427\n",
      "[293]\ttraining's binary_logloss: 0.398634\ttraining's auc: 0.844313\n",
      "[294]\ttraining's binary_logloss: 0.398547\ttraining's auc: 0.844378\n",
      "[295]\ttraining's binary_logloss: 0.398438\ttraining's auc: 0.844465\n",
      "[296]\ttraining's binary_logloss: 0.39841\ttraining's auc: 0.844484\n",
      "[297]\ttraining's binary_logloss: 0.398344\ttraining's auc: 0.844521\n",
      "[298]\ttraining's binary_logloss: 0.398293\ttraining's auc: 0.844562\n",
      "[299]\ttraining's binary_logloss: 0.398263\ttraining's auc: 0.844581\n",
      "[300]\ttraining's binary_logloss: 0.398212\ttraining's auc: 0.844614\n",
      "[301]\ttraining's binary_logloss: 0.398126\ttraining's auc: 0.844717\n",
      "[302]\ttraining's binary_logloss: 0.398077\ttraining's auc: 0.844743\n",
      "[303]\ttraining's binary_logloss: 0.39802\ttraining's auc: 0.844771\n",
      "[304]\ttraining's binary_logloss: 0.397988\ttraining's auc: 0.844805\n",
      "[305]\ttraining's binary_logloss: 0.397891\ttraining's auc: 0.844889\n",
      "[306]\ttraining's binary_logloss: 0.39782\ttraining's auc: 0.844937\n",
      "[307]\ttraining's binary_logloss: 0.39774\ttraining's auc: 0.845024\n",
      "[308]\ttraining's binary_logloss: 0.397676\ttraining's auc: 0.845067\n",
      "[309]\ttraining's binary_logloss: 0.397626\ttraining's auc: 0.845119\n",
      "[310]\ttraining's binary_logloss: 0.397542\ttraining's auc: 0.845191\n",
      "[311]\ttraining's binary_logloss: 0.39752\ttraining's auc: 0.845206\n",
      "[312]\ttraining's binary_logloss: 0.397483\ttraining's auc: 0.845235\n",
      "[313]\ttraining's binary_logloss: 0.397442\ttraining's auc: 0.845253\n",
      "[314]\ttraining's binary_logloss: 0.397415\ttraining's auc: 0.845286\n",
      "[315]\ttraining's binary_logloss: 0.397397\ttraining's auc: 0.845297\n",
      "[316]\ttraining's binary_logloss: 0.397379\ttraining's auc: 0.845304\n",
      "[317]\ttraining's binary_logloss: 0.397304\ttraining's auc: 0.845381\n",
      "[318]\ttraining's binary_logloss: 0.397219\ttraining's auc: 0.845493\n",
      "[319]\ttraining's binary_logloss: 0.397109\ttraining's auc: 0.845576\n",
      "[320]\ttraining's binary_logloss: 0.397022\ttraining's auc: 0.845708\n",
      "[321]\ttraining's binary_logloss: 0.396938\ttraining's auc: 0.845809\n",
      "[322]\ttraining's binary_logloss: 0.396844\ttraining's auc: 0.84588\n",
      "[323]\ttraining's binary_logloss: 0.396751\ttraining's auc: 0.84596\n",
      "[324]\ttraining's binary_logloss: 0.396658\ttraining's auc: 0.846079\n",
      "[325]\ttraining's binary_logloss: 0.396583\ttraining's auc: 0.846129\n",
      "[326]\ttraining's binary_logloss: 0.396507\ttraining's auc: 0.846164\n",
      "[327]\ttraining's binary_logloss: 0.396421\ttraining's auc: 0.84621\n",
      "[328]\ttraining's binary_logloss: 0.396332\ttraining's auc: 0.846305\n",
      "[329]\ttraining's binary_logloss: 0.39631\ttraining's auc: 0.846322\n",
      "[330]\ttraining's binary_logloss: 0.396219\ttraining's auc: 0.846389\n",
      "[331]\ttraining's binary_logloss: 0.39612\ttraining's auc: 0.846484\n",
      "[332]\ttraining's binary_logloss: 0.396075\ttraining's auc: 0.846499\n",
      "[333]\ttraining's binary_logloss: 0.396004\ttraining's auc: 0.846532\n",
      "[334]\ttraining's binary_logloss: 0.395973\ttraining's auc: 0.846543\n",
      "[335]\ttraining's binary_logloss: 0.395967\ttraining's auc: 0.846549\n",
      "[336]\ttraining's binary_logloss: 0.395958\ttraining's auc: 0.846554\n",
      "[337]\ttraining's binary_logloss: 0.395927\ttraining's auc: 0.846598\n",
      "[338]\ttraining's binary_logloss: 0.395825\ttraining's auc: 0.846711\n",
      "[339]\ttraining's binary_logloss: 0.395728\ttraining's auc: 0.846818\n",
      "[340]\ttraining's binary_logloss: 0.395655\ttraining's auc: 0.846889\n",
      "[341]\ttraining's binary_logloss: 0.395572\ttraining's auc: 0.846938\n",
      "[342]\ttraining's binary_logloss: 0.39547\ttraining's auc: 0.847017\n",
      "[343]\ttraining's binary_logloss: 0.395447\ttraining's auc: 0.847045\n",
      "[344]\ttraining's binary_logloss: 0.395409\ttraining's auc: 0.847062\n",
      "[345]\ttraining's binary_logloss: 0.395332\ttraining's auc: 0.847103\n",
      "[346]\ttraining's binary_logloss: 0.395251\ttraining's auc: 0.847188\n",
      "[347]\ttraining's binary_logloss: 0.395239\ttraining's auc: 0.847194\n",
      "[348]\ttraining's binary_logloss: 0.395203\ttraining's auc: 0.847213\n",
      "[349]\ttraining's binary_logloss: 0.395116\ttraining's auc: 0.847281\n",
      "[350]\ttraining's binary_logloss: 0.395037\ttraining's auc: 0.847386\n",
      "[351]\ttraining's binary_logloss: 0.394948\ttraining's auc: 0.847486\n",
      "[352]\ttraining's binary_logloss: 0.394878\ttraining's auc: 0.847556\n",
      "[353]\ttraining's binary_logloss: 0.394851\ttraining's auc: 0.847583\n",
      "[354]\ttraining's binary_logloss: 0.394822\ttraining's auc: 0.847627\n",
      "[355]\ttraining's binary_logloss: 0.394793\ttraining's auc: 0.847641\n",
      "[356]\ttraining's binary_logloss: 0.394709\ttraining's auc: 0.847729\n",
      "[357]\ttraining's binary_logloss: 0.394698\ttraining's auc: 0.847735\n",
      "[358]\ttraining's binary_logloss: 0.394604\ttraining's auc: 0.847825\n",
      "[359]\ttraining's binary_logloss: 0.394519\ttraining's auc: 0.847911\n",
      "[360]\ttraining's binary_logloss: 0.394441\ttraining's auc: 0.847973\n",
      "[361]\ttraining's binary_logloss: 0.394371\ttraining's auc: 0.848047\n",
      "[362]\ttraining's binary_logloss: 0.394282\ttraining's auc: 0.848142\n",
      "[363]\ttraining's binary_logloss: 0.394179\ttraining's auc: 0.848257\n",
      "[364]\ttraining's binary_logloss: 0.39408\ttraining's auc: 0.848387\n",
      "[365]\ttraining's binary_logloss: 0.394023\ttraining's auc: 0.848484\n",
      "[366]\ttraining's binary_logloss: 0.393986\ttraining's auc: 0.84853\n",
      "[367]\ttraining's binary_logloss: 0.393898\ttraining's auc: 0.848614\n",
      "[368]\ttraining's binary_logloss: 0.393882\ttraining's auc: 0.848626\n",
      "[369]\ttraining's binary_logloss: 0.39379\ttraining's auc: 0.848694\n",
      "[370]\ttraining's binary_logloss: 0.393704\ttraining's auc: 0.848786\n",
      "[371]\ttraining's binary_logloss: 0.393614\ttraining's auc: 0.848837\n",
      "[372]\ttraining's binary_logloss: 0.393537\ttraining's auc: 0.848933\n",
      "[373]\ttraining's binary_logloss: 0.393455\ttraining's auc: 0.848971\n",
      "[374]\ttraining's binary_logloss: 0.393422\ttraining's auc: 0.849012\n",
      "[375]\ttraining's binary_logloss: 0.393354\ttraining's auc: 0.849093\n",
      "[376]\ttraining's binary_logloss: 0.393321\ttraining's auc: 0.849131\n",
      "[377]\ttraining's binary_logloss: 0.393234\ttraining's auc: 0.849198\n",
      "[378]\ttraining's binary_logloss: 0.393141\ttraining's auc: 0.84928\n",
      "[379]\ttraining's binary_logloss: 0.393047\ttraining's auc: 0.849396\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[380]\ttraining's binary_logloss: 0.392956\ttraining's auc: 0.849465\n",
      "[381]\ttraining's binary_logloss: 0.392861\ttraining's auc: 0.849572\n",
      "[382]\ttraining's binary_logloss: 0.392773\ttraining's auc: 0.849683\n",
      "[383]\ttraining's binary_logloss: 0.392676\ttraining's auc: 0.849753\n",
      "[384]\ttraining's binary_logloss: 0.392577\ttraining's auc: 0.849827\n",
      "[385]\ttraining's binary_logloss: 0.392561\ttraining's auc: 0.849835\n",
      "[386]\ttraining's binary_logloss: 0.392471\ttraining's auc: 0.849908\n",
      "[387]\ttraining's binary_logloss: 0.392419\ttraining's auc: 0.84993\n",
      "[388]\ttraining's binary_logloss: 0.392407\ttraining's auc: 0.849935\n",
      "[389]\ttraining's binary_logloss: 0.392381\ttraining's auc: 0.849946\n",
      "[390]\ttraining's binary_logloss: 0.3923\ttraining's auc: 0.850039\n",
      "[391]\ttraining's binary_logloss: 0.392216\ttraining's auc: 0.850125\n",
      "[392]\ttraining's binary_logloss: 0.392135\ttraining's auc: 0.85022\n",
      "[393]\ttraining's binary_logloss: 0.392114\ttraining's auc: 0.850246\n",
      "[394]\ttraining's binary_logloss: 0.39203\ttraining's auc: 0.850311\n",
      "[395]\ttraining's binary_logloss: 0.391999\ttraining's auc: 0.850326\n",
      "[396]\ttraining's binary_logloss: 0.391989\ttraining's auc: 0.850335\n",
      "[397]\ttraining's binary_logloss: 0.391881\ttraining's auc: 0.850464\n",
      "[398]\ttraining's binary_logloss: 0.391784\ttraining's auc: 0.850554\n",
      "[399]\ttraining's binary_logloss: 0.391691\ttraining's auc: 0.850652\n",
      "[400]\ttraining's binary_logloss: 0.391664\ttraining's auc: 0.85066\n",
      "[401]\ttraining's binary_logloss: 0.391586\ttraining's auc: 0.850694\n",
      "[402]\ttraining's binary_logloss: 0.39151\ttraining's auc: 0.850758\n",
      "[403]\ttraining's binary_logloss: 0.391424\ttraining's auc: 0.850862\n",
      "[404]\ttraining's binary_logloss: 0.391334\ttraining's auc: 0.850897\n",
      "[405]\ttraining's binary_logloss: 0.391232\ttraining's auc: 0.85099\n",
      "[406]\ttraining's binary_logloss: 0.39115\ttraining's auc: 0.851061\n",
      "[407]\ttraining's binary_logloss: 0.391073\ttraining's auc: 0.851099\n",
      "[408]\ttraining's binary_logloss: 0.39098\ttraining's auc: 0.851204\n",
      "[409]\ttraining's binary_logloss: 0.390912\ttraining's auc: 0.851226\n",
      "[410]\ttraining's binary_logloss: 0.390821\ttraining's auc: 0.851262\n",
      "[411]\ttraining's binary_logloss: 0.390742\ttraining's auc: 0.851301\n",
      "[412]\ttraining's binary_logloss: 0.390686\ttraining's auc: 0.851398\n",
      "[413]\ttraining's binary_logloss: 0.390659\ttraining's auc: 0.851412\n",
      "[414]\ttraining's binary_logloss: 0.390612\ttraining's auc: 0.851453\n",
      "[415]\ttraining's binary_logloss: 0.390544\ttraining's auc: 0.851477\n",
      "[416]\ttraining's binary_logloss: 0.39053\ttraining's auc: 0.851487\n",
      "[417]\ttraining's binary_logloss: 0.390441\ttraining's auc: 0.851597\n",
      "[418]\ttraining's binary_logloss: 0.390416\ttraining's auc: 0.851615\n",
      "[419]\ttraining's binary_logloss: 0.390351\ttraining's auc: 0.851686\n",
      "[420]\ttraining's binary_logloss: 0.390291\ttraining's auc: 0.85174\n",
      "[421]\ttraining's binary_logloss: 0.390232\ttraining's auc: 0.851764\n",
      "[422]\ttraining's binary_logloss: 0.390204\ttraining's auc: 0.851781\n",
      "[423]\ttraining's binary_logloss: 0.390124\ttraining's auc: 0.851871\n",
      "[424]\ttraining's binary_logloss: 0.390021\ttraining's auc: 0.851938\n",
      "[425]\ttraining's binary_logloss: 0.389944\ttraining's auc: 0.851959\n",
      "[426]\ttraining's binary_logloss: 0.389874\ttraining's auc: 0.852037\n",
      "[427]\ttraining's binary_logloss: 0.389803\ttraining's auc: 0.852116\n",
      "[428]\ttraining's binary_logloss: 0.389741\ttraining's auc: 0.852144\n",
      "[429]\ttraining's binary_logloss: 0.389705\ttraining's auc: 0.852166\n",
      "[430]\ttraining's binary_logloss: 0.389619\ttraining's auc: 0.852252\n",
      "[431]\ttraining's binary_logloss: 0.389557\ttraining's auc: 0.852273\n",
      "[432]\ttraining's binary_logloss: 0.389475\ttraining's auc: 0.852367\n",
      "[433]\ttraining's binary_logloss: 0.389407\ttraining's auc: 0.852425\n",
      "[434]\ttraining's binary_logloss: 0.389362\ttraining's auc: 0.852476\n",
      "[435]\ttraining's binary_logloss: 0.389299\ttraining's auc: 0.852543\n",
      "[436]\ttraining's binary_logloss: 0.389234\ttraining's auc: 0.852646\n",
      "[437]\ttraining's binary_logloss: 0.389153\ttraining's auc: 0.852705\n",
      "[438]\ttraining's binary_logloss: 0.389112\ttraining's auc: 0.852732\n",
      "[439]\ttraining's binary_logloss: 0.389003\ttraining's auc: 0.852847\n",
      "[440]\ttraining's binary_logloss: 0.388943\ttraining's auc: 0.852899\n",
      "[441]\ttraining's binary_logloss: 0.388849\ttraining's auc: 0.853016\n",
      "[442]\ttraining's binary_logloss: 0.388749\ttraining's auc: 0.853078\n",
      "[443]\ttraining's binary_logloss: 0.388698\ttraining's auc: 0.853123\n",
      "[444]\ttraining's binary_logloss: 0.388655\ttraining's auc: 0.853143\n",
      "[445]\ttraining's binary_logloss: 0.388611\ttraining's auc: 0.853192\n",
      "[446]\ttraining's binary_logloss: 0.388519\ttraining's auc: 0.853276\n",
      "[447]\ttraining's binary_logloss: 0.388445\ttraining's auc: 0.85333\n",
      "[448]\ttraining's binary_logloss: 0.38836\ttraining's auc: 0.853379\n",
      "[449]\ttraining's binary_logloss: 0.388254\ttraining's auc: 0.853519\n",
      "[450]\ttraining's binary_logloss: 0.388174\ttraining's auc: 0.853604\n",
      "[451]\ttraining's binary_logloss: 0.388086\ttraining's auc: 0.853709\n",
      "[452]\ttraining's binary_logloss: 0.388056\ttraining's auc: 0.853719\n",
      "[453]\ttraining's binary_logloss: 0.388031\ttraining's auc: 0.853761\n",
      "[454]\ttraining's binary_logloss: 0.387983\ttraining's auc: 0.853797\n",
      "[455]\ttraining's binary_logloss: 0.387889\ttraining's auc: 0.853888\n",
      "[456]\ttraining's binary_logloss: 0.387799\ttraining's auc: 0.853994\n",
      "[457]\ttraining's binary_logloss: 0.38773\ttraining's auc: 0.854049\n",
      "[458]\ttraining's binary_logloss: 0.387646\ttraining's auc: 0.854113\n",
      "[459]\ttraining's binary_logloss: 0.387609\ttraining's auc: 0.854125\n",
      "[460]\ttraining's binary_logloss: 0.387581\ttraining's auc: 0.854144\n",
      "[461]\ttraining's binary_logloss: 0.387505\ttraining's auc: 0.854195\n",
      "[462]\ttraining's binary_logloss: 0.387436\ttraining's auc: 0.854222\n",
      "[463]\ttraining's binary_logloss: 0.387404\ttraining's auc: 0.854233\n",
      "[464]\ttraining's binary_logloss: 0.387329\ttraining's auc: 0.854315\n",
      "[465]\ttraining's binary_logloss: 0.387233\ttraining's auc: 0.854414\n",
      "[466]\ttraining's binary_logloss: 0.387151\ttraining's auc: 0.85446\n",
      "[467]\ttraining's binary_logloss: 0.387114\ttraining's auc: 0.854478\n",
      "[468]\ttraining's binary_logloss: 0.387047\ttraining's auc: 0.854551\n",
      "[469]\ttraining's binary_logloss: 0.386968\ttraining's auc: 0.854649\n",
      "[470]\ttraining's binary_logloss: 0.386951\ttraining's auc: 0.854658\n",
      "[471]\ttraining's binary_logloss: 0.386925\ttraining's auc: 0.854682\n",
      "[472]\ttraining's binary_logloss: 0.386856\ttraining's auc: 0.854707\n",
      "[473]\ttraining's binary_logloss: 0.38679\ttraining's auc: 0.854729\n",
      "[474]\ttraining's binary_logloss: 0.386705\ttraining's auc: 0.854845\n",
      "[475]\ttraining's binary_logloss: 0.386631\ttraining's auc: 0.85489\n",
      "[476]\ttraining's binary_logloss: 0.38656\ttraining's auc: 0.854943\n",
      "[477]\ttraining's binary_logloss: 0.38651\ttraining's auc: 0.854964\n",
      "[478]\ttraining's binary_logloss: 0.386425\ttraining's auc: 0.855059\n",
      "[479]\ttraining's binary_logloss: 0.386322\ttraining's auc: 0.855177\n",
      "[480]\ttraining's binary_logloss: 0.386256\ttraining's auc: 0.855262\n",
      "[481]\ttraining's binary_logloss: 0.386251\ttraining's auc: 0.855267\n",
      "[482]\ttraining's binary_logloss: 0.386218\ttraining's auc: 0.855283\n",
      "[483]\ttraining's binary_logloss: 0.386163\ttraining's auc: 0.855381\n",
      "[484]\ttraining's binary_logloss: 0.386136\ttraining's auc: 0.855392\n",
      "[485]\ttraining's binary_logloss: 0.386084\ttraining's auc: 0.855414\n",
      "[486]\ttraining's binary_logloss: 0.38602\ttraining's auc: 0.855445\n",
      "[487]\ttraining's binary_logloss: 0.385955\ttraining's auc: 0.855522\n",
      "[488]\ttraining's binary_logloss: 0.385901\ttraining's auc: 0.855558\n",
      "[489]\ttraining's binary_logloss: 0.385802\ttraining's auc: 0.855666\n",
      "[490]\ttraining's binary_logloss: 0.385714\ttraining's auc: 0.855718\n",
      "[491]\ttraining's binary_logloss: 0.385627\ttraining's auc: 0.855798\n",
      "[492]\ttraining's binary_logloss: 0.385535\ttraining's auc: 0.855908\n",
      "[493]\ttraining's binary_logloss: 0.385479\ttraining's auc: 0.855968\n",
      "[494]\ttraining's binary_logloss: 0.385388\ttraining's auc: 0.856063\n",
      "[495]\ttraining's binary_logloss: 0.385358\ttraining's auc: 0.856086\n",
      "[496]\ttraining's binary_logloss: 0.385281\ttraining's auc: 0.856191\n",
      "[497]\ttraining's binary_logloss: 0.385211\ttraining's auc: 0.856282\n",
      "[498]\ttraining's binary_logloss: 0.385134\ttraining's auc: 0.85633\n",
      "[499]\ttraining's binary_logloss: 0.385051\ttraining's auc: 0.856433\n",
      "[500]\ttraining's binary_logloss: 0.385008\ttraining's auc: 0.85647\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[500]\ttraining's binary_logloss: 0.385008\ttraining's auc: 0.85647\n",
      "Best: 0.826934 using {'num_leaves': 80, 'n_estimators': 500, 'min_child_weight': 5, 'max_depth': 9, 'learning_rate': 0.05, 'colsample_bytree': 0.6000000000000001}\n"
     ]
    }
   ],
   "source": [
    "def execute_pipeline():\n",
    "    \n",
    "    df = read_data()\n",
    "    df = reduce_mem_usage(df, True)\n",
    "    df = transform_data(df)\n",
    "    df = feature_engineering(df, break_point)\n",
    "    run_lgb(df)\n",
    "\n",
    "execute_pipeline()"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
