{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Objective of the notebook:\n",
    "\n",
    "In this notebook, we create a machine learning model pipeline to predict probabilities and calculate model scores.\n",
    "- Importing libraries\n",
    "- Importing datasets\n",
    "- Change datatypes and reduce memory usage\n",
    "- Label encode categorical features (Features already encoded for this task.)\n",
    "- Fill null values\n",
    "- Convert raw data to a session format\n",
    "- Run the xgboost model\n",
    "- Calculate model scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-12T17:55:40.395024Z",
     "start_time": "2020-10-12T17:55:37.285440Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kaan.simsek/anaconda3/envs/forecasting/lib/python3.7/site-packages/lightgbm/__init__.py:46: UserWarning: Starting from version 2.2.1, the library file in distribution wheels for macOS is built by the Apple Clang (Xcode_8.3.3) compiler.\n",
      "This means that in case of installing LightGBM from PyPI via the ``pip install lightgbm`` command, you don't need to install the gcc compiler anymore.\n",
      "Instead of that, you need to install the OpenMP library, which is required for running LightGBM on the system with the Apple Clang compiler.\n",
      "You can install the OpenMP library by the following command: ``brew install libomp``.\n",
      "  \"You can install the OpenMP library by the following command: ``brew install libomp``.\", UserWarning)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta \n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import cross_val_predict, cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "import xgboost\n",
    "import lightgbm \n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\") \n",
    "\n",
    "kfold = KFold(n_splits=5, random_state=42)\n",
    "break_point = datetime(2017, 2, 28)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-12T17:55:40.406065Z",
     "start_time": "2020-10-12T17:55:40.398125Z"
    }
   },
   "outputs": [],
   "source": [
    "def read_data():\n",
    "    \n",
    "    print('Reading files...')    \n",
    "    order_df = pd.read_csv('../input/machine_learning_challenge_order_data.csv')\n",
    "    print('Order data has {} rows and {} columns'.format(order_df.shape[0], order_df.shape[1]))\n",
    "    label_df = pd.read_csv('../input/machine_learning_challenge_labeled_data.csv')\n",
    "    print('Label data has {} rows and {} columns'.format(label_df.shape[0], label_df.shape[1]))\n",
    "    df = order_df.merge(label_df, on='customer_id')\n",
    "    print('The final data has {} rows and {} columns'.format(df.shape[0], df.shape[1]))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Change data types and reduce memory usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-12T17:55:40.420604Z",
     "start_time": "2020-10-12T17:55:40.409081Z"
    }
   },
   "outputs": [],
   "source": [
    "def reduce_mem_usage(df, verbose=False):\n",
    "    \n",
    "    start_mem = df.memory_usage().sum() / 1024 ** 2\n",
    "    int_columns = df.select_dtypes(include=[\"int\"]).columns\n",
    "    float_columns = df.select_dtypes(include=[\"float\"]).columns\n",
    "\n",
    "    for col in int_columns:\n",
    "        df[col] = pd.to_numeric(df[col], downcast=\"integer\")\n",
    "\n",
    "    for col in float_columns:\n",
    "        df[col] = pd.to_numeric(df[col], downcast=\"float\")\n",
    "\n",
    "    end_mem = df.memory_usage().sum() / 1024 ** 2\n",
    "    if verbose:\n",
    "        print(\n",
    "            \"Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)\".format(\n",
    "                end_mem, 100 * (start_mem - end_mem) / start_mem\n",
    "            )\n",
    "        )\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Label encode categorical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-12T17:55:40.429883Z",
     "start_time": "2020-10-12T17:55:40.424128Z"
    }
   },
   "outputs": [],
   "source": [
    "def transform_data(df):\n",
    "\n",
    "    labelencoder = LabelEncoder()\n",
    "\n",
    "    for i in ['restaurant_id', 'city_id', 'payment_id', 'platform_id', 'transmission_id']:\n",
    "        df[i] = labelencoder.fit_transform(df[i])\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert raw data to a session format\n",
    "Fill the order rank with the forward-filling method. As a baseline model, we only keep the last record of each customer. Because we assume that the last record of a user gives more info such as recency, order count, cancel, etc.)\n",
    "\n",
    "For modeling, we chose the tree-based xgboost model because it is easy to implement and ready for production in cloud services and also more successful in tabular datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-12T17:55:40.439179Z",
     "start_time": "2020-10-12T17:55:40.433478Z"
    }
   },
   "outputs": [],
   "source": [
    "def feature_engineering(df, break_point):\n",
    "\n",
    "    df['customer_order_rank'] = df['customer_order_rank'].fillna(method='ffill')\n",
    "    df = df.groupby('customer_id').last().reset_index()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run xgb model and calculate scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-12T17:55:40.467054Z",
     "start_time": "2020-10-12T17:55:40.442253Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "          n_jobs=None, penalty='l2', random_state=42, solver='warn',\n",
       "          tol=0.0001, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "LogisticRegression(random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-12T18:29:04.011564Z",
     "start_time": "2020-10-12T18:29:04.005014Z"
    }
   },
   "outputs": [],
   "source": [
    "classifiers=['KNN', 'Decision Tree','RandomForestClassifier', \n",
    "             'Logistic Regression','XGBoostClassifier', 'LGBMClaffier']\n",
    "\n",
    "models = [KNeighborsClassifier(), \n",
    "       DecisionTreeClassifier(random_state=42),\n",
    "       RandomForestClassifier(random_state=42), \n",
    "       LogisticRegression(random_state=42),\n",
    "       xgboost.XGBClassifier(random_state=42), \n",
    "       lightgbm.LGBMClassifier()]\n",
    "\n",
    "def cross_val(X, Y):\n",
    "    \n",
    "    auc_mean=[]\n",
    "    auc_std=[]\n",
    "    acc_mean=[]\n",
    "    acc_std=[]\n",
    "    \n",
    "    for model in models:\n",
    "        cv_result = cross_val_score(model, X, Y, cv = kfold, scoring = \"roc_auc\")\n",
    "        auc_mean.append(cv_result.mean())\n",
    "        auc_std.append(cv_result.std())\n",
    "\n",
    "        cv_result = cross_val_score(model, X, Y, cv = kfold, scoring = \"accuracy\")\n",
    "        acc_mean.append(cv_result.mean())\n",
    "        acc_std.append(cv_result.std())\n",
    "\n",
    "    results=round(pd.DataFrame({'Roc-Auc Mean':auc_mean,'Roc-Auc Std':auc_std,\n",
    "                                'Accuracy Mean':acc_mean, 'Accuracy Std':acc_std}, index=classifiers), 4)  \n",
    "    print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-12T18:29:04.464417Z",
     "start_time": "2020-10-12T18:29:04.458369Z"
    }
   },
   "outputs": [],
   "source": [
    "def run_xgb(df):\n",
    "    \n",
    "    y = df['is_returning_customer']\n",
    "    X = df.drop(columns=['customer_id', 'order_date', 'is_returning_customer'])    \n",
    "    \n",
    "    cross_val(X,y)\n",
    "\n",
    "    clf = xgboost.XGBClassifier(objective= 'binary:logistic', n_jobs= -1, scale_pos_weight=2)\n",
    "    \n",
    "    y_pred = cross_val_predict(clf, X, y, cv=kfold)\n",
    "    \n",
    "    print('Accuracy Score:  ',round(metrics.accuracy_score(y, y_pred), 2))\n",
    "    print('Roc Auc Score:  ',round(roc_auc_score(y, y_pred), 2))\n",
    "    print('Classification Report: \\n', classification_report(y, y_pred, target_names=['0', '1']))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execute all pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-10-12T18:29:05.662Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading files...\n",
      "Order data has 786600 rows and 13 columns\n",
      "Label data has 245455 rows and 2 columns\n",
      "The final data has 786600 rows and 14 columns\n",
      "\n",
      "Mem. usage decreased to 42.76 Mb (52.5% reduction)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def execute_pipeline():\n",
    "    \n",
    "    df = read_data()\n",
    "    print()\n",
    "    df = reduce_mem_usage(df, True)\n",
    "    df = transform_data(df)\n",
    "    df = feature_engineering(df, break_point)\n",
    "    print()\n",
    "    run_xgb(df)\n",
    "    \n",
    "execute_pipeline()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is next? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy and roc-auc scores are 0.8 and 0.7 relatively. \n",
    "\n",
    "We give more weights on class 1 using the scale pos weight parameter to label returned customers more correctly. In other words, we weight roc-auc rather than accuracy.\n",
    "\n",
    "We will add recency and time-related features for the new version and compare model results with the baseline model."
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
